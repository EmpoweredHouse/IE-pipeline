{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be9add11",
   "metadata": {},
   "source": [
    "# MRDA Dialogue Act Classification Pipeline\n",
    "\n",
    "## Multi-Stage Training Approach:\n",
    "1. **Stage 1:** Train 12-class General DA classifier \n",
    "2. **Stage 2:** Map to binary content/non-content classification\n",
    "\n",
    "**Target Model Repository:** `wylupek/distilbert-mrda-dialogue-acts`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0b6267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/filipkozlowski/Documents/github/IE-pipeline/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import platform\n",
    "import psutil\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from huggingface_hub import login, whoami\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d95a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: Darwin 23.6.0\n",
      "Architecture: arm64\n",
      "CPU Cores: 8\n",
      "RAM: 16.0 GB\n",
      "MPS Device: Apple Silicon (MPS)\n",
      "Unified Memory Available\n"
     ]
    }
   ],
   "source": [
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"Architecture: {platform.machine()}\")\n",
    "print(f\"CPU Cores: {psutil.cpu_count()}\")\n",
    "print(f\"RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "\n",
    "\n",
    "def detect_device():\n",
    "    \"\"\"Detect best available device with fallback strategy\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        device_name = torch.cuda.get_device_name(0)\n",
    "        memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        print(f\"CUDA Device: {device_name}\")\n",
    "        print(f\"GPU Memory: {memory_gb:.1f} GB\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps\" \n",
    "        device_name = \"Apple Silicon (MPS)\"\n",
    "        print(f\"MPS Device: {device_name}\")\n",
    "        print(f\"Unified Memory Available\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        device_name = \"CPU\"\n",
    "        print(f\"CPU Device: {device_name}\")\n",
    "        print(f\"Using CPU cores: {psutil.cpu_count()}\")\n",
    "    \n",
    "    return device, device_name\n",
    "\n",
    "device, device_name = detect_device()\n",
    "\n",
    "# Test device\n",
    "try:\n",
    "    test_tensor = torch.randn(10, 10).to(device)\n",
    "    result = torch.matmul(test_tensor, test_tensor.T)\n",
    "    del test_tensor, result\n",
    "except Exception as e:\n",
    "    print(f\"Device Test Failed: {e}\")\n",
    "    print(\"Falling back to CPU...\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0253782d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits:\n",
      "  train: 75,067 samples\n",
      "  test: 16,702 samples\n",
      "  validation: 16,433 samples\n",
      "  Total: 108,202 samples\n",
      "\n",
      "Sample data: {'speaker': 'fe016', 'text': 'okay.', 'basic_da': 'F', 'general_da': 'fg', 'full_da': 'fg'}\n",
      "Unique general_da labels: 12\n",
      "Labels: ['%', 'b', 'fg', 'fh', 'h', 'qh', 'qo', 'qr', 'qrr', 'qw', 'qy', 's']\n",
      "\n",
      "Label Distribution (Training Set):\n",
      "  %: 2,171 samples (2.9%)\n",
      "  b: 10,606 samples (14.1%)\n",
      "  fg: 2,076 samples (2.8%)\n",
      "  fh: 5,617 samples (7.5%)\n",
      "  h: 474 samples (0.6%)\n",
      "  qh: 260 samples (0.3%)\n",
      "  qo: 116 samples (0.2%)\n",
      "  qr: 131 samples (0.2%)\n",
      "  qrr: 244 samples (0.3%)\n",
      "  qw: 1,110 samples (1.5%)\n",
      "  qy: 3,310 samples (4.4%)\n",
      "  s: 48,952 samples (65.2%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"wylupek/mrda-corpus\")\n",
    "\n",
    "print(f\"Dataset splits:\")\n",
    "for split_name, split_data in dataset.items():\n",
    "    print(f\"  {split_name}: {len(split_data):,} samples\")\n",
    "total_samples = sum(len(split) for split in dataset.values())\n",
    "print(f\"  Total: {total_samples:,} samples\\n\")\n",
    "\n",
    "print(f\"Sample data: {dataset['train'][0]}\")\n",
    "\n",
    "\n",
    "train_labels = [sample['general_da'] for sample in dataset['train']]\n",
    "unique_labels = list(set(train_labels))\n",
    "unique_labels.sort()\n",
    "print(f\"Unique general_da labels: {len(unique_labels)}\")\n",
    "print(f\"Labels: {unique_labels}\\n\")\n",
    "\n",
    "\n",
    "label_counts = pd.Series(train_labels).value_counts().sort_index()\n",
    "print(f\"Label Distribution (Training Set):\")\n",
    "for label, count in label_counts.items():\n",
    "    percentage = (count / len(train_labels)) * 100\n",
    "    print(f\"  {label}: {count:,} samples ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a79a2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split:\n",
      "  Content: 54,123 samples (72.1%)\n",
      "  Non-content: 20,944 samples (27.9%)\n",
      "Validation split:\n",
      "  Content: 11,715 samples (71.3%)\n",
      "  Non-content: 4,718 samples (28.7%)\n",
      "Test split:\n",
      "  Content: 11,848 samples (70.9%)\n",
      "  Non-content: 4,854 samples (29.1%)\n"
     ]
    }
   ],
   "source": [
    "CONTENT_LABELS = {\n",
    "    's',      # Statement (65.2% - main content)\n",
    "    'qy',     # Yes-No-question (4.4%)\n",
    "    'qw',     # Wh-Question (1.5%)\n",
    "    'qh',     # Rhetorical Question (0.3%)\n",
    "    'qrr',    # Or-Clause (0.3%)\n",
    "    'qr',     # Or Question (0.2%)\n",
    "    'qo'      # Open-ended Question (0.2%)\n",
    "}\n",
    "\n",
    "NON_CONTENT_LABELS = {\n",
    "    'b',      # Continuer (14.1% - backchannels)\n",
    "    'fh',     # Floor Holder (7.5% - floor management)\n",
    "    'fg',     # Floor Grabber (2.8% - floor management)\n",
    "    '%',      # Interrupted/Abandoned (2.9% - disruptions)\n",
    "    'h'       # Hold Before Answer (0.6% - hesitations)\n",
    "}\n",
    "\n",
    "def calculate_content_distribution(labels):\n",
    "    \"\"\"Calculate content vs non-content percentages\"\"\"\n",
    "    content_count = sum(1 for label in labels if label in CONTENT_LABELS)\n",
    "    non_content_count = sum(1 for label in labels if label in NON_CONTENT_LABELS)\n",
    "    total = len(labels)\n",
    "    \n",
    "    content_pct = (content_count / total) * 100\n",
    "    non_content_pct = (non_content_count / total) * 100\n",
    "    \n",
    "    return content_count, non_content_count, content_pct, non_content_pct\n",
    "\n",
    "def map_to_binary(general_da_label):\n",
    "    \"\"\"Map general DA label to binary content/non-content\"\"\"\n",
    "    if general_da_label in CONTENT_LABELS:\n",
    "        return 1  # Content\n",
    "    elif general_da_label in NON_CONTENT_LABELS:\n",
    "        return 0  # Non-content\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label: {general_da_label}\")\n",
    "\n",
    "def map_to_text(general_da_label):\n",
    "    \"\"\"Map general DA label to text description\"\"\"\n",
    "    if general_da_label in CONTENT_LABELS:\n",
    "        return \"content\"\n",
    "    elif general_da_label in NON_CONTENT_LABELS:\n",
    "        return \"non-content\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label: {general_da_label}\")\n",
    "\n",
    "\n",
    "for split_name in ['train', 'validation', 'test']:\n",
    "    split_labels = [sample['general_da'] for sample in dataset[split_name]]\n",
    "    content_count, non_content_count, content_pct, non_content_pct = calculate_content_distribution(split_labels)\n",
    "    \n",
    "    print(f\"{split_name.capitalize()} split:\")\n",
    "    print(f\"  Content: {content_count:,} samples ({content_pct:.1f}%)\")\n",
    "    print(f\"  Non-content: {non_content_count:,} samples ({non_content_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "132962f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample text: 'okay so um i was going to try to get out of here'\n",
      "Tokenized shape: torch.Size([1, 15])\n",
      "Tokens: [101, 3100, 2061, 8529, 1045, 2001, 2183, 2000, 3046, 2000, 2131, 2041, 1997, 2182, 102]\n",
      "\n",
      "Logits shape: torch.Size([1, 12])\n",
      "Predictions: [0.08603396266698837, 0.08311612904071808, 0.08861508965492249, 0.07143370807170868, 0.07717002928256989, 0.09354806691408157, 0.08418650180101395, 0.08463449776172638, 0.08989214897155762, 0.08497928082942963, 0.07637372612953186, 0.0800168365240097]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "num_labels = 12\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Test tokenization\n",
    "sample_text = \"okay so um i was going to try to get out of here\"\n",
    "encoded = tokenizer(sample_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(f\"\\nSample text: '{sample_text}'\")\n",
    "print(f\"Tokenized shape: {encoded['input_ids'].shape}\")\n",
    "print(f\"Tokens: {encoded['input_ids'][0].tolist()}\\n\")\n",
    "\n",
    "\n",
    "encoded = encoded.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Predictions: {predictions.tolist()[0]}\")\n",
    "\n",
    "del encoded, outputs, logits, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03248c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping:\n",
      "  % -> 0\n",
      "  b -> 1\n",
      "  fg -> 2\n",
      "  fh -> 3\n",
      "  h -> 4\n",
      "  qh -> 5\n",
      "  qo -> 6\n",
      "  qr -> 7\n",
      "  qrr -> 8\n",
      "  qw -> 9\n",
      "  qy -> 10\n",
      "  s -> 11\n",
      "\n",
      "Max length: 96\n"
     ]
    }
   ],
   "source": [
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(\"Label mapping:\")\n",
    "for label, idx in label2id.items():\n",
    "    print(f\"  {label} -> {idx}\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize text and encode labels\"\"\"\n",
    "    tokens = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,  # Will pad later in DataCollator\n",
    "        max_length=128,  # Actual max length is 96\n",
    "        return_tensors=None\n",
    "    )\n",
    "    tokens[\"labels\"] = [label2id[label] for label in examples[\"general_da\"]]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to all splits\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "# Verify no data loss\n",
    "if len(dataset[\"train\"]) != len(tokenized_datasets[\"train\"]):\n",
    "    print(f\"Not all samples were processed\\n\")\n",
    "    print(f\"Original train size: {len(dataset['train']):,}\")\n",
    "    print(f\"Processed train size: {len(tokenized_datasets['train']):,}\")\n",
    "if len(dataset[\"validation\"]) != len(tokenized_datasets[\"validation\"]):\n",
    "    print(f\"Not all samples were processed\\n\")\n",
    "    print(f\"Original validation size: {len(dataset['validation']):,}\")\n",
    "    print(f\"Processed validation size: {len(tokenized_datasets['validation']):,}\")\n",
    "if len(dataset[\"test\"]) != len(tokenized_datasets[\"test\"]):\n",
    "    print(f\"Not all samples were processed\\n\")\n",
    "    print(f\"Original test size: {len(dataset['test']):,}\")\n",
    "    print(f\"Processed test size: {len(tokenized_datasets['test']):,}\")\n",
    "\n",
    "# Check max length\n",
    "print(\"\\nMax length:\", max(max(len(s[\"input_ids\"]) for s in tokenized_datasets[split]) for split in [\"train\",\"validation\",\"test\"]))\n",
    "\n",
    "# Create data collator for batching\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b1166bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights (balanced method):\n",
      "  0 (%): 2.881 (count: 2171)\n",
      "  1 (b): 0.590 (count: 10606)\n",
      "  2 (fg): 3.013 (count: 2076)\n",
      "  3 (fh): 1.114 (count: 5617)\n",
      "  4 (h): 13.197 (count: 474)\n",
      "  5 (qh): 24.060 (count: 260)\n",
      "  6 (qo): 50.000 (count: 116)\n",
      "  7 (qr): 47.753 (count: 131)\n",
      "  8 (qrr): 25.638 (count: 244)\n",
      "  9 (qw): 5.636 (count: 1110)\n",
      "  10 (qy): 1.890 (count: 3310)\n",
      "  11 (s): 0.128 (count: 48952)\n"
     ]
    }
   ],
   "source": [
    "# 1. Calculate class weights (sklearn balanced method)\n",
    "train_labels = [sample[\"labels\"] for sample in tokenized_datasets[\"train\"]]\n",
    "unique_classes_in_subset = sorted(set(train_labels))\n",
    "\n",
    "# Compute weights only for classes present in subset\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array(unique_classes_in_subset),\n",
    "    y=np.array(train_labels)\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# max weight = 50, min weight = 0.5\n",
    "class_weights_tensor[class_weights_tensor > 50] = 50\n",
    "class_weights_tensor[class_weights_tensor < 0.1] = 0.1\n",
    "\n",
    "print(f\"Class weights (balanced method):\")\n",
    "for class_id in unique_classes_in_subset:\n",
    "    weight = class_weights_tensor[class_id].item()\n",
    "    label_name = id2label[class_id]\n",
    "    count = train_labels.count(class_id)\n",
    "    print(f\"  {class_id} ({label_name}): {weight:.3f} (count: {count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99bce13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal loss with balanced cross entropy\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Custom Weighted Trainer\n",
    "class AdvancedTrainer(Trainer):\n",
    "    def __init__(self, loss_type=\"focal\", class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_type = loss_type\n",
    "        self.class_weights = class_weights\n",
    "        \n",
    "        if loss_type == \"focal\":\n",
    "            self.loss_fn = FocalLoss(alpha=class_weights, gamma=2.0)\n",
    "        elif loss_type == \"weighted_ce\":\n",
    "            self.loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        else:\n",
    "            self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        loss = self.loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
    "    micro_f1 = f1_score(labels, predictions, average='micro')\n",
    "    weighted_f1 = f1_score(labels, predictions, average='weighted')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"weighted_f1\": weighted_f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "115113c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.00046061722708429296, 1: 9.428625306430322e-05, 2: 0.0004816955684007707, 3: 0.00017803097739006588, 4: 0.002109704641350211, 5: 0.0038461538461538464, 6: 0.008620689655172414, 7: 0.007633587786259542, 8: 0.004098360655737705, 9: 0.0009009009009009009, 10: 0.00030211480362537764, 11: 2.0428174538323256e-05}\n"
     ]
    }
   ],
   "source": [
    "# TODO the trainer isn't using stratified sampling\n",
    "# 4. Stratified Batch Sampling\n",
    "def create_stratified_sampler(dataset):\n",
    "    # Get labels from dataset\n",
    "    labels = [sample[\"labels\"] for sample in dataset]\n",
    "    \n",
    "    # Calculate sample weights for stratified sampling - handle non-contiguous labels\n",
    "    unique_labels_in_data = sorted(set(labels))\n",
    "    class_sample_count = {label: labels.count(label) for label in unique_labels_in_data}\n",
    "    \n",
    "    # Create weight mapping for each class\n",
    "    weight_mapping = {}\n",
    "    for label, count in class_sample_count.items():\n",
    "        weight_mapping[label] = 1.0 / count\n",
    "    \n",
    "    # Assign weight to each sample\n",
    "    samples_weight = torch.tensor([weight_mapping[label] for label in labels])\n",
    "    print(weight_mapping)\n",
    "    # Create sampler\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=samples_weight,\n",
    "        num_samples=len(samples_weight),\n",
    "        replacement=True\n",
    "    )\n",
    "    return sampler\n",
    "\n",
    "# Create stratified sampler\n",
    "train_sampler = create_stratified_sampler(tokenized_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff815852",
   "metadata": {},
   "source": [
    "### Parameters tuning\n",
    "**LoRA**\n",
    "- lora_dropout – Regular dropout regularization, prevents overfitting. (0.0; 0.3)\n",
    "- lora_r – The size of LoRA adapters, means how much new information the model can learn. High values cause better learning capacity, especially usefull for imbalanced data  (8; 64)\n",
    "- lora_alpha – How strong the LoRA adapters influence the original model. Affects effective scaling (`lora_alpha/lora_r`) (scale between 0.5 and 8)\n",
    "\n",
    "**Traning arguments**\n",
    "\n",
    "NOTE: Step is calculated by `ceil(total_samples / batch_size)`\n",
    "\n",
    "Proper Warmup Guidelines:\n",
    "- Simple tasks: 5-10% of total steps\n",
    "- Complex tasks: 10-15% of total steps\n",
    "- Imbalanced/Difficult: 15-20% of total steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e2e9e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LoRA setup ###\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    target_modules=[\"q_lin\", \"v_lin\", \"k_lin\", \"out_lin\"],\n",
    "    ### TUNABLE PARAMETERS ###\n",
    "    lora_dropout=0.15,\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    ")\n",
    "advanced_peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "### Training setup ###\n",
    "no_epochs = 10\n",
    "batch_size = 24\n",
    "total_steps = no_epochs * np.ceil(len(tokenized_datasets[\"train\"]) / batch_size)\n",
    "advanced_training_args = TrainingArguments(\n",
    "    output_dir=\"./advanced_checkpoints\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",  # Optimize for macro F1, perfect for imbalanced data\n",
    "    greater_is_better=True,\n",
    "    report_to=None,\n",
    "    remove_unused_columns=False, # Required for custom trainer\n",
    "    dataloader_num_workers=0,  # Important for MPS compatibility\n",
    "    ### TUNABLE PARAMETERS ###\n",
    "    learning_rate=0.0002,\n",
    "    num_train_epochs=no_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=int(0.15 * total_steps), # Prevents overfitting, should be ~15% of total steps for imbalanced data\n",
    "    save_steps=total_steps // no_epochs // 2, # 2 times per epoch\n",
    "    eval_steps=total_steps // no_epochs // 8, # 8 times per epoch\n",
    "    logging_steps=total_steps // no_epochs // 10, # 10 times per epoch\n",
    ")\n",
    "advanced_trainer = AdvancedTrainer(\n",
    "    loss_type=\"focal\",  # Focal loss for imbalanced data\n",
    "    class_weights=class_weights_tensor, # Weights for imbalanced data\n",
    "    model=advanced_peft_model,\n",
    "    args=advanced_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d3c93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nStarting advanced training with:\")\n",
    "print(f\"  Loss: Focal Loss (gamma=2.0, class weights)\")\n",
    "print(f\"  Evaluation: Macro F1, Micro F1, Weighted F1, Accuracy\")\n",
    "print(f\"  Model: DistilBERT + LoRA + Advanced Loss\")\n",
    "\n",
    "# Run advanced training\n",
    "advanced_result = advanced_trainer.train()\n",
    "\n",
    "print(f\"\\nAdvanced training completed!\")\n",
    "print(f\"Final train loss: {advanced_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b212de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation\n",
    "advanced_eval = advanced_trainer.evaluate()\n",
    "print(f\"\\nAdvanced evaluation results:\")\n",
    "print(f\"  Accuracy: {advanced_eval['eval_accuracy']:.4f}\")\n",
    "print(f\"  Macro F1: {advanced_eval['eval_macro_f1']:.4f}\")\n",
    "print(f\"  Micro F1: {advanced_eval['eval_micro_f1']:.4f}\")\n",
    "print(f\"  Weighted F1: {advanced_eval['eval_weighted_f1']:.4f}\")\n",
    "\n",
    "# Check prediction diversity\n",
    "print(f\"\\nPrediction diversity analysis:\")\n",
    "advanced_predictions = advanced_trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predicted_classes = np.argmax(advanced_predictions.predictions, axis=1)\n",
    "unique_predictions = len(set(predicted_classes))\n",
    "predicted_counts = pd.Series(predicted_classes).value_counts().sort_index()\n",
    "\n",
    "print(f\"  Unique classes predicted: {unique_predictions}/12\")\n",
    "print(f\"  Prediction distribution:\")\n",
    "for label_id, count in predicted_counts.items():\n",
    "    label_name = id2label[label_id]\n",
    "    percentage = (count / len(predicted_classes)) * 100\n",
    "    print(f\"    {label_id} ({label_name}): {count} predictions ({percentage:.1f}%)\")\n",
    "\n",
    "# Compare with baseline (Step 6 results)\n",
    "print(f\"\\nComparison with baseline:\")\n",
    "print(f\"  Baseline accuracy: 65.0% (predicting only 1 class)\")\n",
    "print(f\"  Advanced accuracy: {advanced_eval['eval_accuracy']*100:.1f}%\")\n",
    "print(f\"  Baseline macro F1: ~0.08 (random)\")\n",
    "print(f\"  Advanced macro F1: {advanced_eval['eval_macro_f1']:.3f}\")\n",
    "print(f\"  Prediction diversity: {unique_predictions}/12 classes vs 1/12 baseline\")\n",
    "\n",
    "if advanced_eval['eval_macro_f1'] > 0.3:\n",
    "    print(\"✅ Advanced training SUCCESS! Macro F1 > 0.3\")\n",
    "elif unique_predictions > 5:\n",
    "    print(\"✅ Good prediction diversity! Learning multiple classes\")\n",
    "else:\n",
    "    print(\"⚠️  May need further tuning - try weighted_ce loss or adjust gamma\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
