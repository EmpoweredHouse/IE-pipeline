{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be9add11",
   "metadata": {},
   "source": [
    "# MRDA Dialogue Act Classification Pipeline\n",
    "\n",
    "## Multi-Stage Training Approach:\n",
    "1. **Stage 1:** Train 12-class General DA classifier \n",
    "2. **Stage 2:** Map to binary content/non-content classification\n",
    "\n",
    "**Target Model Repository:** `wylupek/distilbert-mrda-dialogue-acts`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0b6267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filip/Documents/github/IE-pipeline/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import platform\n",
    "import psutil\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from huggingface_hub import login, whoami\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d95a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: Linux 5.15.0-151-generic\n",
      "Architecture: x86_64\n",
      "CPU Cores: 24\n",
      "RAM: 31.2 GB\n",
      "CUDA Device: NVIDIA GeForce RTX 3060 Ti\n",
      "GPU Memory: 7.8 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"Architecture: {platform.machine()}\")\n",
    "print(f\"CPU Cores: {psutil.cpu_count()}\")\n",
    "print(f\"RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "\n",
    "\n",
    "def detect_device():\n",
    "    \"\"\"Detect best available device with fallback strategy\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        device_name = torch.cuda.get_device_name(0)\n",
    "        memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        print(f\"CUDA Device: {device_name}\")\n",
    "        print(f\"GPU Memory: {memory_gb:.1f} GB\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps\" \n",
    "        device_name = \"Apple Silicon (MPS)\"\n",
    "        print(f\"MPS Device: {device_name}\")\n",
    "        print(f\"Unified Memory Available\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        device_name = \"CPU\"\n",
    "        print(f\"CPU Device: {device_name}\")\n",
    "        print(f\"Using CPU cores: {psutil.cpu_count()}\")\n",
    "    \n",
    "    return device, device_name\n",
    "\n",
    "device, device_name = detect_device()\n",
    "\n",
    "# Test device\n",
    "try:\n",
    "    test_tensor = torch.randn(10, 10).to(device)\n",
    "    result = torch.matmul(test_tensor, test_tensor.T)\n",
    "    del test_tensor, result\n",
    "except Exception as e:\n",
    "    print(f\"Device Test Failed: {e}\")\n",
    "    print(\"Falling back to CPU...\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0253782d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits:\n",
      "  train: 75,067 samples\n",
      "  test: 16,702 samples\n",
      "  validation: 16,433 samples\n",
      "  Total: 108,202 samples\n",
      "\n",
      "Sample data: {'speaker': 'fe016', 'text': 'okay.', 'basic_da': 'F', 'general_da': 'fg', 'full_da': 'fg'}\n",
      "Unique general_da labels: 12\n",
      "Labels: ['%', 'b', 'fg', 'fh', 'h', 'qh', 'qo', 'qr', 'qrr', 'qw', 'qy', 's']\n",
      "\n",
      "Label Distribution (Training Set):\n",
      "  %: 440 samples (2.7%)\n",
      "  b: 2,342 samples (14.3%)\n",
      "  fg: 527 samples (3.2%)\n",
      "  fh: 1,225 samples (7.5%)\n",
      "  h: 184 samples (1.1%)\n",
      "  qh: 36 samples (0.2%)\n",
      "  qo: 25 samples (0.2%)\n",
      "  qr: 39 samples (0.2%)\n",
      "  qrr: 73 samples (0.4%)\n",
      "  qw: 287 samples (1.7%)\n",
      "  qy: 806 samples (4.9%)\n",
      "  s: 10,449 samples (63.6%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"wylupek/mrda-corpus\")\n",
    "\n",
    "print(f\"Dataset splits:\")\n",
    "for split_name, split_data in dataset.items():\n",
    "    print(f\"  {split_name}: {len(split_data):,} samples\")\n",
    "total_samples = sum(len(split) for split in dataset.values())\n",
    "print(f\"  Total: {total_samples:,} samples\\n\")\n",
    "\n",
    "print(f\"Sample data: {dataset['train'][0]}\")\n",
    "\n",
    "\n",
    "train_labels = [sample['general_da'] for sample in dataset['validation']]\n",
    "unique_labels = list(set(train_labels))\n",
    "unique_labels.sort()\n",
    "print(f\"Unique general_da labels: {len(unique_labels)}\")\n",
    "print(f\"Labels: {unique_labels}\\n\")\n",
    "\n",
    "\n",
    "label_counts = pd.Series(train_labels).value_counts().sort_index()\n",
    "print(f\"Label Distribution (Training Set):\")\n",
    "for label, count in label_counts.items():\n",
    "    percentage = (count / len(train_labels)) * 100\n",
    "    print(f\"  {label}: {count:,} samples ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a79a2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split:\n",
      "  Content: 54,123 samples (72.1%)\n",
      "  Non-content: 20,944 samples (27.9%)\n",
      "Validation split:\n",
      "  Content: 11,715 samples (71.3%)\n",
      "  Non-content: 4,718 samples (28.7%)\n",
      "Test split:\n",
      "  Content: 11,848 samples (70.9%)\n",
      "  Non-content: 4,854 samples (29.1%)\n"
     ]
    }
   ],
   "source": [
    "CONTENT_LABELS = {\n",
    "    's',      # Statement (65.2% - main content)\n",
    "    'qy',     # Yes-No-question (4.4%)\n",
    "    'qw',     # Wh-Question (1.5%)\n",
    "    'qh',     # Rhetorical Question (0.3%)\n",
    "    'qrr',    # Or-Clause (0.3%)\n",
    "    'qr',     # Or Question (0.2%)\n",
    "    'qo'      # Open-ended Question (0.2%)\n",
    "}\n",
    "\n",
    "NON_CONTENT_LABELS = {\n",
    "    'b',      # Continuer (14.1% - backchannels)\n",
    "    'fh',     # Floor Holder (7.5% - floor management)\n",
    "    'fg',     # Floor Grabber (2.8% - floor management)\n",
    "    '%',      # Interrupted/Abandoned (2.9% - disruptions)\n",
    "    'h'       # Hold Before Answer (0.6% - hesitations)\n",
    "}\n",
    "\n",
    "def calculate_content_distribution(labels):\n",
    "    \"\"\"Calculate content vs non-content percentages\"\"\"\n",
    "    content_count = sum(1 for label in labels if label in CONTENT_LABELS)\n",
    "    non_content_count = sum(1 for label in labels if label in NON_CONTENT_LABELS)\n",
    "    total = len(labels)\n",
    "    \n",
    "    content_pct = (content_count / total) * 100\n",
    "    non_content_pct = (non_content_count / total) * 100\n",
    "    \n",
    "    return content_count, non_content_count, content_pct, non_content_pct\n",
    "\n",
    "def map_to_binary(general_da_label):\n",
    "    \"\"\"Map general DA label to binary content/non-content\"\"\"\n",
    "    if general_da_label in CONTENT_LABELS:\n",
    "        return 1  # Content\n",
    "    elif general_da_label in NON_CONTENT_LABELS:\n",
    "        return 0  # Non-content\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label: {general_da_label}\")\n",
    "\n",
    "def map_to_text(general_da_label):\n",
    "    \"\"\"Map general DA label to text description\"\"\"\n",
    "    if general_da_label in CONTENT_LABELS:\n",
    "        return \"content\"\n",
    "    elif general_da_label in NON_CONTENT_LABELS:\n",
    "        return \"non-content\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label: {general_da_label}\")\n",
    "\n",
    "\n",
    "for split_name in ['train', 'validation', 'test']:\n",
    "    split_labels = [sample['general_da'] for sample in dataset[split_name]]\n",
    "    content_count, non_content_count, content_pct, non_content_pct = calculate_content_distribution(split_labels)\n",
    "    \n",
    "    print(f\"{split_name.capitalize()} split:\")\n",
    "    print(f\"  Content: {content_count:,} samples ({content_pct:.1f}%)\")\n",
    "    print(f\"  Non-content: {non_content_count:,} samples ({non_content_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "132962f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample text: 'okay so um i was going to try to get out of here'\n",
      "Tokenized shape: torch.Size([1, 15])\n",
      "Tokens: [101, 3100, 2061, 8529, 1045, 2001, 2183, 2000, 3046, 2000, 2131, 2041, 1997, 2182, 102]\n",
      "\n",
      "Logits shape: torch.Size([1, 12])\n",
      "Predictions: [0.08611761033535004, 0.07528205960988998, 0.09400619566440582, 0.07494442164897919, 0.07367940247058868, 0.08856073021888733, 0.08872058242559433, 0.0765603557229042, 0.09287962317466736, 0.08037149161100388, 0.07952561974525452, 0.08935189247131348]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "num_labels = 12\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Test tokenization\n",
    "sample_text = \"okay so um i was going to try to get out of here\"\n",
    "encoded = tokenizer(sample_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(f\"\\nSample text: '{sample_text}'\")\n",
    "print(f\"Tokenized shape: {encoded['input_ids'].shape}\")\n",
    "print(f\"Tokens: {encoded['input_ids'][0].tolist()}\\n\")\n",
    "\n",
    "\n",
    "encoded = encoded.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Predictions: {predictions.tolist()[0]}\")\n",
    "\n",
    "del encoded, outputs, logits, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03248c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping:\n",
      "  % -> 0\n",
      "  b -> 1\n",
      "  fg -> 2\n",
      "  fh -> 3\n",
      "  h -> 4\n",
      "  qh -> 5\n",
      "  qo -> 6\n",
      "  qr -> 7\n",
      "  qrr -> 8\n",
      "  qw -> 9\n",
      "  qy -> 10\n",
      "  s -> 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 75067/75067 [00:00<00:00, 88378.53 examples/s]\n",
      "Tokenizing: 100%|██████████| 16702/16702 [00:00<00:00, 65932.55 examples/s]\n",
      "Tokenizing: 100%|██████████| 16433/16433 [00:00<00:00, 131969.18 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Max length: 96\n"
     ]
    }
   ],
   "source": [
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(\"Label mapping:\")\n",
    "for label, idx in label2id.items():\n",
    "    print(f\"  {label} -> {idx}\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize text and encode labels\"\"\"\n",
    "    tokens = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,  # Will pad later in DataCollator\n",
    "        max_length=128,  # Actual max length is 96\n",
    "        return_tensors=None\n",
    "    )\n",
    "    tokens[\"labels\"] = [label2id[label] for label in examples[\"general_da\"]]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to all splits\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "# Verify no data loss\n",
    "if len(dataset[\"train\"]) != len(tokenized_datasets[\"train\"]):\n",
    "    print(f\"Not all samples were processed\\n\")\n",
    "    print(f\"Original train size: {len(dataset['train']):,}\")\n",
    "    print(f\"Processed train size: {len(tokenized_datasets['train']):,}\")\n",
    "if len(dataset[\"validation\"]) != len(tokenized_datasets[\"validation\"]):\n",
    "    print(f\"Not all samples were processed\\n\")\n",
    "    print(f\"Original validation size: {len(dataset['validation']):,}\")\n",
    "    print(f\"Processed validation size: {len(tokenized_datasets['validation']):,}\")\n",
    "if len(dataset[\"test\"]) != len(tokenized_datasets[\"test\"]):\n",
    "    print(f\"Not all samples were processed\\n\")\n",
    "    print(f\"Original test size: {len(dataset['test']):,}\")\n",
    "    print(f\"Processed test size: {len(tokenized_datasets['test']):,}\")\n",
    "\n",
    "# Check max length\n",
    "print(\"\\nMax length:\", max(max(len(s[\"input_ids\"]) for s in tokenized_datasets[split]) for split in [\"train\",\"validation\",\"test\"]))\n",
    "\n",
    "# Create data collator for batching\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b1166bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights (balanced method):\n",
      "  0 (%): 2.881 (count: 2171)\n",
      "  1 (b): 0.590 (count: 10606)\n",
      "  2 (fg): 3.013 (count: 2076)\n",
      "  3 (fh): 1.114 (count: 5617)\n",
      "  4 (h): 13.197 (count: 474)\n",
      "  5 (qh): 24.060 (count: 260)\n",
      "  6 (qo): 50.000 (count: 116)\n",
      "  7 (qr): 47.753 (count: 131)\n",
      "  8 (qrr): 25.638 (count: 244)\n",
      "  9 (qw): 5.636 (count: 1110)\n",
      "  10 (qy): 1.890 (count: 3310)\n",
      "  11 (s): 0.128 (count: 48952)\n"
     ]
    }
   ],
   "source": [
    "# 1. Calculate class weights (sklearn balanced method)\n",
    "train_labels = [sample[\"labels\"] for sample in tokenized_datasets[\"train\"]]\n",
    "unique_classes_in_subset = sorted(set(train_labels))\n",
    "\n",
    "# Compute weights only for classes present in subset\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array(unique_classes_in_subset),\n",
    "    y=np.array(train_labels)\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# max weight = 50, min weight = 0.5\n",
    "class_weights_tensor[class_weights_tensor > 50] = 50\n",
    "class_weights_tensor[class_weights_tensor < 0.1] = 0.1\n",
    "\n",
    "print(f\"Class weights (balanced method):\")\n",
    "for class_id in unique_classes_in_subset:\n",
    "    weight = class_weights_tensor[class_id].item()\n",
    "    label_name = id2label[class_id]\n",
    "    count = train_labels.count(class_id)\n",
    "    print(f\"  {class_id} ({label_name}): {weight:.3f} (count: {count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99bce13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal loss with balanced cross entropy\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Custom Weighted Trainer\n",
    "class AdvancedTrainer(Trainer):\n",
    "    def __init__(self, loss_type=\"focal\", class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_type = loss_type\n",
    "        self.class_weights = class_weights\n",
    "        \n",
    "        if loss_type == \"focal\":\n",
    "            self.loss_fn = FocalLoss(alpha=class_weights, gamma=2.0)\n",
    "        elif loss_type == \"weighted_ce\":\n",
    "            self.loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        else:\n",
    "            self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        loss = self.loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
    "    micro_f1 = f1_score(labels, predictions, average='micro')\n",
    "    weighted_f1 = f1_score(labels, predictions, average='weighted')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"weighted_f1\": weighted_f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "115113c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.00046061722708429296, 1: 9.428625306430322e-05, 2: 0.0004816955684007707, 3: 0.00017803097739006588, 4: 0.002109704641350211, 5: 0.0038461538461538464, 6: 0.008620689655172414, 7: 0.007633587786259542, 8: 0.004098360655737705, 9: 0.0009009009009009009, 10: 0.00030211480362537764, 11: 2.0428174538323256e-05}\n"
     ]
    }
   ],
   "source": [
    "# TODO the trainer isn't using stratified sampling\n",
    "# 4. Stratified Batch Sampling\n",
    "def create_stratified_sampler(dataset):\n",
    "    # Get labels from dataset\n",
    "    labels = [sample[\"labels\"] for sample in dataset]\n",
    "    \n",
    "    # Calculate sample weights for stratified sampling - handle non-contiguous labels\n",
    "    unique_labels_in_data = sorted(set(labels))\n",
    "    class_sample_count = {label: labels.count(label) for label in unique_labels_in_data}\n",
    "    \n",
    "    # Create weight mapping for each class\n",
    "    weight_mapping = {}\n",
    "    for label, count in class_sample_count.items():\n",
    "        weight_mapping[label] = 1.0 / count\n",
    "    \n",
    "    # Assign weight to each sample\n",
    "    samples_weight = torch.tensor([weight_mapping[label] for label in labels])\n",
    "    print(weight_mapping)\n",
    "    # Create sampler\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=samples_weight,\n",
    "        num_samples=len(samples_weight),\n",
    "        replacement=True\n",
    "    )\n",
    "    return sampler\n",
    "\n",
    "# Create stratified sampler\n",
    "train_sampler = create_stratified_sampler(tokenized_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff815852",
   "metadata": {},
   "source": [
    "### Parameters tuning\n",
    "**LoRA**\n",
    "- lora_dropout – Regular dropout regularization, prevents overfitting. (0.0; 0.3)\n",
    "- lora_r – The size of LoRA adapters, means how much new information the model can learn. High values cause better learning capacity, especially usefull for imbalanced data  (8; 64)\n",
    "- lora_alpha – How strong the LoRA adapters influence the original model. Affects effective scaling (`lora_alpha/lora_r`) (scale between 0.5 and 8)\n",
    "\n",
    "**Traning arguments**\n",
    "\n",
    "NOTE: Step is calculated by `ceil(total_samples / batch_size)`\n",
    "\n",
    "Proper Warmup Guidelines:\n",
    "- Simple tasks: 5-10% of total steps\n",
    "- Complex tasks: 10-15% of total steps\n",
    "- Imbalanced/Difficult: 15-20% of total steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2e9e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LoRA setup ###\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    target_modules=[\"q_lin\", \"v_lin\", \"k_lin\", \"out_lin\"],\n",
    "    ### TUNABLE PARAMETERS ###\n",
    "    lora_dropout=0.15,\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    ")\n",
    "advanced_peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "### Training setup ###\n",
    "no_epochs = 10\n",
    "batch_size = 24\n",
    "total_steps = no_epochs * np.ceil(len(tokenized_datasets[\"train\"]) / batch_size)\n",
    "advanced_training_args = TrainingArguments(\n",
    "    output_dir=\"./advanced_checkpoints\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",  # Optimize for macro F1, perfect for imbalanced data\n",
    "    greater_is_better=True,\n",
    "    report_to=None,\n",
    "    remove_unused_columns=False, # Required for custom trainer\n",
    "    dataloader_num_workers=0,  # Important for MPS compatibility\n",
    "    ### TUNABLE PARAMETERS ###\n",
    "    learning_rate=0.0002,\n",
    "    num_train_epochs=no_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=int(0.15 * total_steps), # Prevents overfitting, should be ~15% of total steps for imbalanced data\n",
    "    save_steps=total_steps // no_epochs // 2, # 2 times per epoch\n",
    "    eval_steps=total_steps // no_epochs // 8, # 8 times per epoch\n",
    "    logging_steps=total_steps // no_epochs // 10, # 10 times per epoch\n",
    ")\n",
    "advanced_trainer = AdvancedTrainer(\n",
    "    loss_type=\"focal\",  # Focal loss for imbalanced data\n",
    "    class_weights=class_weights_tensor, # Weights for imbalanced data\n",
    "    model=advanced_peft_model,\n",
    "    args=advanced_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(f\"Total steps: {total_steps}\")\n",
    "print(f\"Steps per epoch: {total_steps / no_epochs}\")\n",
    "print(f\"Steps per validation: {total_steps / no_epochs / 8}\")\n",
    "print(f\"Steps per logging: {total_steps / no_epochs / 10}\")\n",
    "print(f\"Steps per saving: {total_steps / no_epochs / 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5d3c93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting advanced training with:\n",
      "  Loss: Focal Loss (gamma=2.0, class weights)\n",
      "  Evaluation: Macro F1, Micro F1, Weighted F1, Accuracy\n",
      "  Model: DistilBERT + LoRA + Advanced Loss\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31280' max='31280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31280/31280 36:28, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>2.107100</td>\n",
       "      <td>2.326983</td>\n",
       "      <td>0.231364</td>\n",
       "      <td>0.107099</td>\n",
       "      <td>0.231364</td>\n",
       "      <td>0.126118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>1.985500</td>\n",
       "      <td>1.767580</td>\n",
       "      <td>0.268302</td>\n",
       "      <td>0.245786</td>\n",
       "      <td>0.268302</td>\n",
       "      <td>0.202222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1173</td>\n",
       "      <td>1.507000</td>\n",
       "      <td>1.398862</td>\n",
       "      <td>0.316619</td>\n",
       "      <td>0.300849</td>\n",
       "      <td>0.316619</td>\n",
       "      <td>0.266350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1564</td>\n",
       "      <td>1.164500</td>\n",
       "      <td>1.368319</td>\n",
       "      <td>0.412280</td>\n",
       "      <td>0.340447</td>\n",
       "      <td>0.412280</td>\n",
       "      <td>0.431602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1955</td>\n",
       "      <td>1.220800</td>\n",
       "      <td>1.164877</td>\n",
       "      <td>0.421895</td>\n",
       "      <td>0.381829</td>\n",
       "      <td>0.421895</td>\n",
       "      <td>0.427245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2346</td>\n",
       "      <td>1.126100</td>\n",
       "      <td>1.011669</td>\n",
       "      <td>0.480071</td>\n",
       "      <td>0.390435</td>\n",
       "      <td>0.480071</td>\n",
       "      <td>0.516566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2737</td>\n",
       "      <td>0.976300</td>\n",
       "      <td>1.144527</td>\n",
       "      <td>0.541350</td>\n",
       "      <td>0.412392</td>\n",
       "      <td>0.541350</td>\n",
       "      <td>0.565047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3128</td>\n",
       "      <td>1.002500</td>\n",
       "      <td>1.027956</td>\n",
       "      <td>0.467839</td>\n",
       "      <td>0.386774</td>\n",
       "      <td>0.467839</td>\n",
       "      <td>0.508069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3519</td>\n",
       "      <td>1.050700</td>\n",
       "      <td>1.229010</td>\n",
       "      <td>0.485243</td>\n",
       "      <td>0.397888</td>\n",
       "      <td>0.485243</td>\n",
       "      <td>0.504832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3910</td>\n",
       "      <td>1.072000</td>\n",
       "      <td>1.324551</td>\n",
       "      <td>0.521755</td>\n",
       "      <td>0.389648</td>\n",
       "      <td>0.521755</td>\n",
       "      <td>0.541535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4301</td>\n",
       "      <td>1.033000</td>\n",
       "      <td>1.064808</td>\n",
       "      <td>0.363476</td>\n",
       "      <td>0.402805</td>\n",
       "      <td>0.363476</td>\n",
       "      <td>0.410621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4692</td>\n",
       "      <td>1.100100</td>\n",
       "      <td>1.073817</td>\n",
       "      <td>0.544636</td>\n",
       "      <td>0.444057</td>\n",
       "      <td>0.544636</td>\n",
       "      <td>0.565639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5083</td>\n",
       "      <td>0.948500</td>\n",
       "      <td>1.180538</td>\n",
       "      <td>0.497901</td>\n",
       "      <td>0.420617</td>\n",
       "      <td>0.497901</td>\n",
       "      <td>0.509700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5474</td>\n",
       "      <td>1.062900</td>\n",
       "      <td>1.174128</td>\n",
       "      <td>0.531187</td>\n",
       "      <td>0.480808</td>\n",
       "      <td>0.531187</td>\n",
       "      <td>0.578255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5865</td>\n",
       "      <td>1.045400</td>\n",
       "      <td>1.080786</td>\n",
       "      <td>0.353496</td>\n",
       "      <td>0.416001</td>\n",
       "      <td>0.353496</td>\n",
       "      <td>0.344695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6256</td>\n",
       "      <td>0.922200</td>\n",
       "      <td>1.191962</td>\n",
       "      <td>0.554798</td>\n",
       "      <td>0.446667</td>\n",
       "      <td>0.554798</td>\n",
       "      <td>0.573468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6647</td>\n",
       "      <td>1.053400</td>\n",
       "      <td>1.111896</td>\n",
       "      <td>0.510558</td>\n",
       "      <td>0.481263</td>\n",
       "      <td>0.510558</td>\n",
       "      <td>0.536557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7038</td>\n",
       "      <td>0.947300</td>\n",
       "      <td>1.036459</td>\n",
       "      <td>0.456399</td>\n",
       "      <td>0.450404</td>\n",
       "      <td>0.456399</td>\n",
       "      <td>0.475753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7429</td>\n",
       "      <td>0.841700</td>\n",
       "      <td>1.294675</td>\n",
       "      <td>0.542080</td>\n",
       "      <td>0.458580</td>\n",
       "      <td>0.542080</td>\n",
       "      <td>0.571258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7820</td>\n",
       "      <td>0.879900</td>\n",
       "      <td>1.228865</td>\n",
       "      <td>0.535021</td>\n",
       "      <td>0.461420</td>\n",
       "      <td>0.535021</td>\n",
       "      <td>0.560746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8211</td>\n",
       "      <td>1.234000</td>\n",
       "      <td>1.121371</td>\n",
       "      <td>0.415262</td>\n",
       "      <td>0.419618</td>\n",
       "      <td>0.415262</td>\n",
       "      <td>0.417716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8602</td>\n",
       "      <td>0.850300</td>\n",
       "      <td>1.289398</td>\n",
       "      <td>0.490233</td>\n",
       "      <td>0.433706</td>\n",
       "      <td>0.490233</td>\n",
       "      <td>0.501536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8993</td>\n",
       "      <td>0.931000</td>\n",
       "      <td>1.033302</td>\n",
       "      <td>0.556441</td>\n",
       "      <td>0.472751</td>\n",
       "      <td>0.556441</td>\n",
       "      <td>0.592204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9384</td>\n",
       "      <td>0.846200</td>\n",
       "      <td>1.129784</td>\n",
       "      <td>0.509889</td>\n",
       "      <td>0.477369</td>\n",
       "      <td>0.509889</td>\n",
       "      <td>0.536143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9775</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>1.273046</td>\n",
       "      <td>0.572020</td>\n",
       "      <td>0.487118</td>\n",
       "      <td>0.572020</td>\n",
       "      <td>0.595265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10166</td>\n",
       "      <td>0.895100</td>\n",
       "      <td>1.125428</td>\n",
       "      <td>0.497657</td>\n",
       "      <td>0.486198</td>\n",
       "      <td>0.497657</td>\n",
       "      <td>0.529386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10557</td>\n",
       "      <td>0.984800</td>\n",
       "      <td>1.157007</td>\n",
       "      <td>0.496805</td>\n",
       "      <td>0.474023</td>\n",
       "      <td>0.496805</td>\n",
       "      <td>0.530084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10948</td>\n",
       "      <td>0.689800</td>\n",
       "      <td>1.283352</td>\n",
       "      <td>0.548896</td>\n",
       "      <td>0.498097</td>\n",
       "      <td>0.548896</td>\n",
       "      <td>0.579826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11339</td>\n",
       "      <td>0.854300</td>\n",
       "      <td>1.184382</td>\n",
       "      <td>0.441307</td>\n",
       "      <td>0.439924</td>\n",
       "      <td>0.441307</td>\n",
       "      <td>0.443141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11730</td>\n",
       "      <td>0.839200</td>\n",
       "      <td>1.348610</td>\n",
       "      <td>0.527232</td>\n",
       "      <td>0.472621</td>\n",
       "      <td>0.527232</td>\n",
       "      <td>0.556765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12121</td>\n",
       "      <td>0.879000</td>\n",
       "      <td>1.226016</td>\n",
       "      <td>0.458103</td>\n",
       "      <td>0.454982</td>\n",
       "      <td>0.458103</td>\n",
       "      <td>0.468781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12512</td>\n",
       "      <td>0.848700</td>\n",
       "      <td>1.059440</td>\n",
       "      <td>0.506481</td>\n",
       "      <td>0.481163</td>\n",
       "      <td>0.506481</td>\n",
       "      <td>0.539165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12903</td>\n",
       "      <td>0.794600</td>\n",
       "      <td>1.292583</td>\n",
       "      <td>0.535021</td>\n",
       "      <td>0.491487</td>\n",
       "      <td>0.535021</td>\n",
       "      <td>0.556890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13294</td>\n",
       "      <td>0.830500</td>\n",
       "      <td>1.075652</td>\n",
       "      <td>0.538611</td>\n",
       "      <td>0.486698</td>\n",
       "      <td>0.538611</td>\n",
       "      <td>0.576346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13685</td>\n",
       "      <td>0.930900</td>\n",
       "      <td>1.205049</td>\n",
       "      <td>0.526867</td>\n",
       "      <td>0.481249</td>\n",
       "      <td>0.526867</td>\n",
       "      <td>0.559171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14076</td>\n",
       "      <td>0.738300</td>\n",
       "      <td>1.164678</td>\n",
       "      <td>0.529240</td>\n",
       "      <td>0.501011</td>\n",
       "      <td>0.529240</td>\n",
       "      <td>0.567297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14467</td>\n",
       "      <td>0.856400</td>\n",
       "      <td>1.320631</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>0.496790</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>0.564007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14858</td>\n",
       "      <td>0.616300</td>\n",
       "      <td>1.223202</td>\n",
       "      <td>0.588693</td>\n",
       "      <td>0.515122</td>\n",
       "      <td>0.588693</td>\n",
       "      <td>0.627177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15249</td>\n",
       "      <td>0.760600</td>\n",
       "      <td>1.237240</td>\n",
       "      <td>0.502039</td>\n",
       "      <td>0.482844</td>\n",
       "      <td>0.502039</td>\n",
       "      <td>0.531873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15640</td>\n",
       "      <td>0.831700</td>\n",
       "      <td>1.427581</td>\n",
       "      <td>0.505142</td>\n",
       "      <td>0.470810</td>\n",
       "      <td>0.505142</td>\n",
       "      <td>0.530686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16031</td>\n",
       "      <td>0.684400</td>\n",
       "      <td>1.237008</td>\n",
       "      <td>0.563013</td>\n",
       "      <td>0.508289</td>\n",
       "      <td>0.563013</td>\n",
       "      <td>0.612004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16422</td>\n",
       "      <td>0.675400</td>\n",
       "      <td>1.278619</td>\n",
       "      <td>0.502891</td>\n",
       "      <td>0.456601</td>\n",
       "      <td>0.502891</td>\n",
       "      <td>0.517160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16813</td>\n",
       "      <td>0.646600</td>\n",
       "      <td>1.196443</td>\n",
       "      <td>0.589180</td>\n",
       "      <td>0.511585</td>\n",
       "      <td>0.589180</td>\n",
       "      <td>0.633108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17204</td>\n",
       "      <td>0.582500</td>\n",
       "      <td>1.303640</td>\n",
       "      <td>0.505568</td>\n",
       "      <td>0.493794</td>\n",
       "      <td>0.505568</td>\n",
       "      <td>0.535835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17595</td>\n",
       "      <td>0.696300</td>\n",
       "      <td>1.375123</td>\n",
       "      <td>0.489320</td>\n",
       "      <td>0.473030</td>\n",
       "      <td>0.489320</td>\n",
       "      <td>0.511985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17986</td>\n",
       "      <td>0.673000</td>\n",
       "      <td>1.279889</td>\n",
       "      <td>0.490111</td>\n",
       "      <td>0.492499</td>\n",
       "      <td>0.490111</td>\n",
       "      <td>0.512876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18377</td>\n",
       "      <td>0.753800</td>\n",
       "      <td>1.343500</td>\n",
       "      <td>0.467900</td>\n",
       "      <td>0.490218</td>\n",
       "      <td>0.467900</td>\n",
       "      <td>0.498611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18768</td>\n",
       "      <td>0.722100</td>\n",
       "      <td>1.244441</td>\n",
       "      <td>0.486095</td>\n",
       "      <td>0.492675</td>\n",
       "      <td>0.486095</td>\n",
       "      <td>0.508539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19159</td>\n",
       "      <td>0.598200</td>\n",
       "      <td>1.287178</td>\n",
       "      <td>0.507880</td>\n",
       "      <td>0.492419</td>\n",
       "      <td>0.507880</td>\n",
       "      <td>0.539434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19550</td>\n",
       "      <td>0.562000</td>\n",
       "      <td>1.347320</td>\n",
       "      <td>0.552425</td>\n",
       "      <td>0.495960</td>\n",
       "      <td>0.552425</td>\n",
       "      <td>0.591972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19941</td>\n",
       "      <td>0.776400</td>\n",
       "      <td>1.226911</td>\n",
       "      <td>0.536177</td>\n",
       "      <td>0.482514</td>\n",
       "      <td>0.536177</td>\n",
       "      <td>0.577467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20332</td>\n",
       "      <td>0.545800</td>\n",
       "      <td>1.257458</td>\n",
       "      <td>0.507272</td>\n",
       "      <td>0.480479</td>\n",
       "      <td>0.507272</td>\n",
       "      <td>0.538065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20723</td>\n",
       "      <td>0.596300</td>\n",
       "      <td>1.270592</td>\n",
       "      <td>0.521207</td>\n",
       "      <td>0.485896</td>\n",
       "      <td>0.521207</td>\n",
       "      <td>0.550297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21114</td>\n",
       "      <td>0.757400</td>\n",
       "      <td>1.365784</td>\n",
       "      <td>0.485061</td>\n",
       "      <td>0.482860</td>\n",
       "      <td>0.485061</td>\n",
       "      <td>0.517259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21505</td>\n",
       "      <td>0.663500</td>\n",
       "      <td>1.396600</td>\n",
       "      <td>0.535751</td>\n",
       "      <td>0.496582</td>\n",
       "      <td>0.535751</td>\n",
       "      <td>0.570258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21896</td>\n",
       "      <td>0.571000</td>\n",
       "      <td>1.389039</td>\n",
       "      <td>0.540619</td>\n",
       "      <td>0.496397</td>\n",
       "      <td>0.540619</td>\n",
       "      <td>0.569986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22287</td>\n",
       "      <td>0.452600</td>\n",
       "      <td>1.520835</td>\n",
       "      <td>0.564048</td>\n",
       "      <td>0.506075</td>\n",
       "      <td>0.564048</td>\n",
       "      <td>0.601909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22678</td>\n",
       "      <td>0.481800</td>\n",
       "      <td>1.378562</td>\n",
       "      <td>0.513783</td>\n",
       "      <td>0.487825</td>\n",
       "      <td>0.513783</td>\n",
       "      <td>0.547670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23069</td>\n",
       "      <td>0.590500</td>\n",
       "      <td>1.497321</td>\n",
       "      <td>0.519199</td>\n",
       "      <td>0.485151</td>\n",
       "      <td>0.519199</td>\n",
       "      <td>0.548850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23460</td>\n",
       "      <td>0.652500</td>\n",
       "      <td>1.360137</td>\n",
       "      <td>0.534169</td>\n",
       "      <td>0.490202</td>\n",
       "      <td>0.534169</td>\n",
       "      <td>0.563903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23851</td>\n",
       "      <td>0.552200</td>\n",
       "      <td>1.451288</td>\n",
       "      <td>0.479401</td>\n",
       "      <td>0.476650</td>\n",
       "      <td>0.479401</td>\n",
       "      <td>0.502770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24242</td>\n",
       "      <td>0.754200</td>\n",
       "      <td>1.411604</td>\n",
       "      <td>0.549321</td>\n",
       "      <td>0.498472</td>\n",
       "      <td>0.549321</td>\n",
       "      <td>0.586273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24633</td>\n",
       "      <td>0.580400</td>\n",
       "      <td>1.415169</td>\n",
       "      <td>0.527840</td>\n",
       "      <td>0.489137</td>\n",
       "      <td>0.527840</td>\n",
       "      <td>0.558292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25024</td>\n",
       "      <td>0.411800</td>\n",
       "      <td>1.481390</td>\n",
       "      <td>0.536360</td>\n",
       "      <td>0.493026</td>\n",
       "      <td>0.536360</td>\n",
       "      <td>0.569706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25415</td>\n",
       "      <td>0.315800</td>\n",
       "      <td>1.567184</td>\n",
       "      <td>0.566178</td>\n",
       "      <td>0.499690</td>\n",
       "      <td>0.566178</td>\n",
       "      <td>0.602199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25806</td>\n",
       "      <td>0.444200</td>\n",
       "      <td>1.615065</td>\n",
       "      <td>0.554190</td>\n",
       "      <td>0.502787</td>\n",
       "      <td>0.554190</td>\n",
       "      <td>0.588878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26197</td>\n",
       "      <td>0.546000</td>\n",
       "      <td>1.539973</td>\n",
       "      <td>0.505386</td>\n",
       "      <td>0.487772</td>\n",
       "      <td>0.505386</td>\n",
       "      <td>0.536710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26588</td>\n",
       "      <td>0.510300</td>\n",
       "      <td>1.527775</td>\n",
       "      <td>0.533926</td>\n",
       "      <td>0.494983</td>\n",
       "      <td>0.533926</td>\n",
       "      <td>0.568073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26979</td>\n",
       "      <td>0.399400</td>\n",
       "      <td>1.483437</td>\n",
       "      <td>0.534717</td>\n",
       "      <td>0.492076</td>\n",
       "      <td>0.534717</td>\n",
       "      <td>0.570360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27370</td>\n",
       "      <td>0.488600</td>\n",
       "      <td>1.495286</td>\n",
       "      <td>0.523215</td>\n",
       "      <td>0.493444</td>\n",
       "      <td>0.523215</td>\n",
       "      <td>0.557826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27761</td>\n",
       "      <td>0.409700</td>\n",
       "      <td>1.529837</td>\n",
       "      <td>0.547374</td>\n",
       "      <td>0.497365</td>\n",
       "      <td>0.547374</td>\n",
       "      <td>0.583912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28152</td>\n",
       "      <td>0.503600</td>\n",
       "      <td>1.541480</td>\n",
       "      <td>0.542323</td>\n",
       "      <td>0.500503</td>\n",
       "      <td>0.542323</td>\n",
       "      <td>0.577812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28543</td>\n",
       "      <td>0.383500</td>\n",
       "      <td>1.591665</td>\n",
       "      <td>0.546461</td>\n",
       "      <td>0.499056</td>\n",
       "      <td>0.546461</td>\n",
       "      <td>0.581539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28934</td>\n",
       "      <td>0.432700</td>\n",
       "      <td>1.547078</td>\n",
       "      <td>0.530214</td>\n",
       "      <td>0.491635</td>\n",
       "      <td>0.530214</td>\n",
       "      <td>0.564108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29325</td>\n",
       "      <td>0.377200</td>\n",
       "      <td>1.592291</td>\n",
       "      <td>0.541776</td>\n",
       "      <td>0.498489</td>\n",
       "      <td>0.541776</td>\n",
       "      <td>0.575414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29716</td>\n",
       "      <td>0.379800</td>\n",
       "      <td>1.567765</td>\n",
       "      <td>0.554372</td>\n",
       "      <td>0.500093</td>\n",
       "      <td>0.554372</td>\n",
       "      <td>0.589895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30107</td>\n",
       "      <td>0.449900</td>\n",
       "      <td>1.599406</td>\n",
       "      <td>0.561675</td>\n",
       "      <td>0.502961</td>\n",
       "      <td>0.561675</td>\n",
       "      <td>0.598694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30498</td>\n",
       "      <td>0.391000</td>\n",
       "      <td>1.602351</td>\n",
       "      <td>0.546096</td>\n",
       "      <td>0.496180</td>\n",
       "      <td>0.546096</td>\n",
       "      <td>0.580646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30889</td>\n",
       "      <td>0.413500</td>\n",
       "      <td>1.630091</td>\n",
       "      <td>0.541958</td>\n",
       "      <td>0.494393</td>\n",
       "      <td>0.541958</td>\n",
       "      <td>0.575392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31280</td>\n",
       "      <td>0.399400</td>\n",
       "      <td>1.624845</td>\n",
       "      <td>0.544271</td>\n",
       "      <td>0.494387</td>\n",
       "      <td>0.544271</td>\n",
       "      <td>0.578181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advanced training completed!\n",
      "Final train loss: 0.7731\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nStarting advanced training with:\")\n",
    "print(f\"  Loss: Focal Loss (gamma=2.0, class weights)\")\n",
    "print(f\"  Evaluation: Macro F1, Micro F1, Weighted F1, Accuracy\")\n",
    "print(f\"  Model: DistilBERT + LoRA + Advanced Loss\")\n",
    "\n",
    "# Run advanced training\n",
    "advanced_result = advanced_trainer.train()\n",
    "\n",
    "print(f\"\\nAdvanced training completed!\")\n",
    "print(f\"Final train loss: {advanced_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113cf503",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPROVED TRAINING CONFIGURATION ###\n",
    "print(\"=== IMPROVED CONFIGURATION ===\")\n",
    "\n",
    "# 1. More Conservative Class Weights (cap at 10.0, minimum 0.2)\n",
    "improved_class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array(unique_classes_in_subset),\n",
    "    y=np.array(train_labels)\n",
    ")\n",
    "improved_class_weights_tensor = torch.tensor(improved_class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Conservative capping - less extreme\n",
    "improved_class_weights_tensor[improved_class_weights_tensor > 10] = 10.0  # Reduced from 50\n",
    "improved_class_weights_tensor[improved_class_weights_tensor < 0.2] = 0.2   # Increased from 0.1\n",
    "\n",
    "print(f\"Improved class weights (conservative):\")\n",
    "for class_id in unique_classes_in_subset:\n",
    "    weight = improved_class_weights_tensor[class_id].item()\n",
    "    label_name = id2label[class_id]\n",
    "    count = train_labels.count(class_id)\n",
    "    print(f\"  {class_id} ({label_name}): {weight:.3f} (count: {count})\")\n",
    "\n",
    "# 2. Improved LoRA Configuration\n",
    "improved_lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    target_modules=[\"q_lin\", \"v_lin\", \"k_lin\", \"out_lin\"],\n",
    "    # More conservative LoRA settings\n",
    "    lora_dropout=0.2,    # Increased dropout for regularization\n",
    "    r=16,                # Reduced rank - less parameters to overfit\n",
    "    lora_alpha=32,       # Reduced influence (alpha/r = 2.0)\n",
    ")\n",
    "\n",
    "# 3. Fresh model for improved training\n",
    "improved_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"single_label_classification\"\n",
    ").to(device)\n",
    "\n",
    "improved_peft_model = get_peft_model(improved_model, improved_lora_config)\n",
    "\n",
    "# 4. Improved Training Arguments with Regularization\n",
    "no_epochs_improved = 6\n",
    "batch_size_improved = 32\n",
    "total_steps_improved = no_epochs_improved * np.ceil(len(tokenized_datasets[\"train\"]) / batch_size_improved)\n",
    "\n",
    "improved_training_args = TrainingArguments(\n",
    "    output_dir=\"./improved_checkpoints\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"weighted_f1\",  # Better for imbalanced data than macro_f1\n",
    "    greater_is_better=True,\n",
    "    report_to=None,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=0,\n",
    "    \n",
    "    # REGULARIZATION IMPROVEMENTS\n",
    "    learning_rate=0.0001,              # Reduced learning rate\n",
    "    num_train_epochs=no_epochs_improved,\n",
    "    per_device_train_batch_size=batch_size_improved,\n",
    "    per_device_eval_batch_size=batch_size_improved,\n",
    "    weight_decay=0.01,                 # Added weight decay for regularization\n",
    "    warmup_steps=int(0.1 * total_steps_improved), # 10% warmup\n",
    "    \n",
    "    # More frequent evaluation to catch overfitting early\n",
    "    save_steps=int(total_steps_improved // no_epochs_improved // 4), # 4 times per epoch\n",
    "    eval_steps=int(total_steps_improved // no_epochs_improved // 8), # 8 times per epoch\n",
    "    logging_steps=int(total_steps_improved // no_epochs_improved // 16), # 16 times per epoch\n",
    ")\n",
    "\n",
    "# 5. Improved Trainer with Weighted CrossEntropy (instead of Focal Loss)\n",
    "improved_trainer = AdvancedTrainer(\n",
    "    loss_type=\"weighted_ce\",  # Simpler, more stable than focal loss\n",
    "    class_weights=improved_class_weights_tensor,\n",
    "    model=improved_peft_model,\n",
    "    args=improved_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(f\"\\nImproved configuration ready:\")\n",
    "print(f\"  Loss: Weighted CrossEntropy (more stable)\")\n",
    "print(f\"  Class weights: Conservative (max 10.0, min 0.2)\")\n",
    "print(f\"  LoRA: Reduced rank (r=16) with higher dropout (0.2)\")\n",
    "print(f\"  Regularization: Weight decay 0.01, reduced LR\")\n",
    "print(f\"  Epochs: {no_epochs_improved} (instead of 10) to prevent overfitting\")\n",
    "print(f\"  Batch size: {batch_size_improved} (larger for stability)\")\n",
    "print(f\"  Total steps: {int(total_steps_improved)}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 🔧 Issue Analysis & Solutions\n",
    "\n",
    "### Problems Identified:\n",
    "\n",
    "| Issue | Original Setting | Problem | Improved Setting |\n",
    "|-------|-----------------|---------|------------------|\n",
    "| **Class Weights** | 0.128 - 50.0 | Too extreme, model avoids majority class | 0.2 - 10.0 (conservative) |\n",
    "| **Loss Function** | Focal + Weights | Over-penalizes easy examples | Weighted CrossEntropy |\n",
    "| **LoRA Influence** | r=32, α=64 | High rank = more overfitting | r=16, α=32 |\n",
    "| **Regularization** | None | Model memorizes training data | Weight decay 0.01, dropout 0.2 |\n",
    "| **Learning Rate** | 0.0002 | Too high for stable learning | 0.0001 |\n",
    "| **Epochs** | 10 | Too many, clear overfitting after epoch 3 | 6 epochs max |\n",
    "\n",
    "### Why Your Results Make Sense:\n",
    "\n",
    "1. **Class 11 Underrepresentation**: Weight of 0.128 made model \"afraid\" to predict the majority class\n",
    "2. **Overfitting with Poor Performance**: Model memorized wrong patterns due to extreme class imbalance correction\n",
    "3. **Loss Divergence**: Training loss ↓, Validation loss ↑ = Classic overfitting\n",
    "\n",
    "### Expected Improvements:\n",
    "\n",
    "- **Accuracy**: Should improve from 52.9% to ~60-65%\n",
    "- **Class 11 Predictions**: Should increase from 24.4% to ~50-60%\n",
    "- **Validation Loss**: Should plateau instead of increasing\n",
    "- **Macro F1**: Should remain good (~0.50) while improving overall accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba87fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Test the improved configuration\n",
    "# Uncomment and run this cell to train with improved settings\n",
    "\n",
    "# print(f\"\\nStarting IMPROVED training with:\")\n",
    "# print(f\"  Loss: Weighted CrossEntropy (stable)\")\n",
    "# print(f\"  Class weights: Conservative (0.2-10.0 range)\")\n",
    "# print(f\"  Regularization: Weight decay + higher dropout\")\n",
    "# print(f\"  Model: DistilBERT + Conservative LoRA\")\n",
    "\n",
    "# # Run improved training\n",
    "# improved_result = improved_trainer.train()\n",
    "\n",
    "# print(f\"\\nImproved training completed!\")\n",
    "# print(f\"Final train loss: {improved_result.training_loss:.4f}\")\n",
    "\n",
    "# # Evaluate improved model\n",
    "# improved_eval = improved_trainer.evaluate()\n",
    "# print(f\"\\nImproved evaluation results:\")\n",
    "# print(f\"  Accuracy: {improved_eval['eval_accuracy']:.4f}\")\n",
    "# print(f\"  Macro F1: {improved_eval['eval_macro_f1']:.4f}\")\n",
    "# print(f\"  Weighted F1: {improved_eval['eval_weighted_f1']:.4f}\")\n",
    "\n",
    "# # Check if class distribution is more balanced\n",
    "# improved_predictions = improved_trainer.predict(tokenized_datasets[\"validation\"])\n",
    "# improved_predicted_classes = np.argmax(improved_predictions.predictions, axis=1)\n",
    "# improved_predicted_counts = pd.Series(improved_predicted_classes).value_counts().sort_index()\n",
    "\n",
    "# print(f\"\\nImproved prediction distribution:\")\n",
    "# for label_id, count in improved_predicted_counts.items():\n",
    "#     label_name = id2label[label_id]\n",
    "#     percentage = (count / len(improved_predicted_classes)) * 100\n",
    "#     print(f\"    {label_id} ({label_name}): {count} predictions ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"Improved configuration ready. Uncomment above code to train with better settings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b212de9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advanced evaluation results:\n",
      "  Accuracy: 0.5292\n",
      "  Macro F1: 0.5010\n",
      "  Micro F1: 0.5292\n",
      "  Weighted F1: 0.5673\n",
      "\n",
      "Prediction diversity analysis:\n",
      "  Unique classes predicted: 12/12\n",
      "  Prediction distribution:\n",
      "    0 (%): 1202 predictions (7.3%)\n",
      "    1 (b): 3487 predictions (21.2%)\n",
      "    2 (fg): 3216 predictions (19.6%)\n",
      "    3 (fh): 2811 predictions (17.1%)\n",
      "    4 (h): 281 predictions (1.7%)\n",
      "    5 (qh): 99 predictions (0.6%)\n",
      "    6 (qo): 22 predictions (0.1%)\n",
      "    7 (qr): 69 predictions (0.4%)\n",
      "    8 (qrr): 92 predictions (0.6%)\n",
      "    9 (qw): 263 predictions (1.6%)\n",
      "    10 (qy): 874 predictions (5.3%)\n",
      "    11 (s): 4017 predictions (24.4%)\n",
      "\n",
      "Comparison with baseline:\n",
      "  Baseline accuracy: 65.0% (predicting only 1 class)\n",
      "  Advanced accuracy: 52.9%\n",
      "  Baseline macro F1: ~0.08 (random)\n",
      "  Advanced macro F1: 0.501\n",
      "  Prediction diversity: 12/12 classes vs 1/12 baseline\n",
      "✅ Advanced training SUCCESS! Macro F1 > 0.3\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive evaluation\n",
    "advanced_eval = advanced_trainer.evaluate()\n",
    "print(f\"\\nAdvanced evaluation results:\")\n",
    "print(f\"  Accuracy: {advanced_eval['eval_accuracy']:.4f}\")\n",
    "print(f\"  Macro F1: {advanced_eval['eval_macro_f1']:.4f}\")\n",
    "print(f\"  Micro F1: {advanced_eval['eval_micro_f1']:.4f}\")\n",
    "print(f\"  Weighted F1: {advanced_eval['eval_weighted_f1']:.4f}\")\n",
    "\n",
    "# Check prediction diversity\n",
    "print(f\"\\nPrediction diversity analysis:\")\n",
    "advanced_predictions = advanced_trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predicted_classes = np.argmax(advanced_predictions.predictions, axis=1)\n",
    "unique_predictions = len(set(predicted_classes))\n",
    "predicted_counts = pd.Series(predicted_classes).value_counts().sort_index()\n",
    "\n",
    "print(f\"  Unique classes predicted: {unique_predictions}/12\")\n",
    "print(f\"  Prediction distribution:\")\n",
    "for label_id, count in predicted_counts.items():\n",
    "    label_name = id2label[label_id]\n",
    "    percentage = (count / len(predicted_classes)) * 100\n",
    "    print(f\"    {label_id} ({label_name}): {count} predictions ({percentage:.1f}%)\")\n",
    "\n",
    "# Compare with baseline (Step 6 results)\n",
    "print(f\"\\nComparison with baseline:\")\n",
    "print(f\"  Baseline accuracy: 65.0% (predicting only 1 class)\")\n",
    "print(f\"  Advanced accuracy: {advanced_eval['eval_accuracy']*100:.1f}%\")\n",
    "print(f\"  Baseline macro F1: ~0.08 (random)\")\n",
    "print(f\"  Advanced macro F1: {advanced_eval['eval_macro_f1']:.3f}\")\n",
    "print(f\"  Prediction diversity: {unique_predictions}/12 classes vs 1/12 baseline\")\n",
    "\n",
    "if advanced_eval['eval_macro_f1'] > 0.3:\n",
    "    print(\"✅ Advanced training SUCCESS! Macro F1 > 0.3\")\n",
    "elif unique_predictions > 5:\n",
    "    print(\"✅ Good prediction diversity! Learning multiple classes\")\n",
    "else:\n",
    "    print(\"⚠️  May need further tuning - try weighted_ce loss or adjust gamma\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
