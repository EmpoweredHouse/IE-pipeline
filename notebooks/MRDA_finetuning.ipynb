{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be9add11",
   "metadata": {},
   "source": [
    "# MRDA Dialogue Act Classification Pipeline\n",
    "\n",
    "## Multi-Stage Training Approach:\n",
    "1. **Stage 1:** Train 12-class General DA classifier \n",
    "2. **Stage 2:** Map to binary content/non-content classification\n",
    "\n",
    "**Target Model Repository:** `wylupek/distilbert-mrda-dialogue-acts`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0b6267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filip/Documents/github/IE-pipeline/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import psutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "\n",
    "import platform\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    TaskType\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d95a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: Linux 5.15.0-151-generic\n",
      "Architecture: x86_64\n",
      "CPU Cores: 24\n",
      "RAM: 31.2 GB\n",
      "CUDA Device: NVIDIA GeForce RTX 3060 Ti\n",
      "GPU Memory: 7.8 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"Architecture: {platform.machine()}\")\n",
    "print(f\"CPU Cores: {psutil.cpu_count()}\")\n",
    "print(f\"RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "\n",
    "\n",
    "def detect_device():\n",
    "    \"\"\"Detect best available device with fallback strategy\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        device_name = torch.cuda.get_device_name(0)\n",
    "        memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        print(f\"CUDA Device: {device_name}\")\n",
    "        print(f\"GPU Memory: {memory_gb:.1f} GB\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps\" \n",
    "        device_name = \"Apple Silicon (MPS)\"\n",
    "        print(f\"MPS Device: {device_name}\")\n",
    "        print(f\"Unified Memory Available\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        device_name = \"CPU\"\n",
    "        print(f\"CPU Device: {device_name}\")\n",
    "        print(f\"Using CPU cores: {psutil.cpu_count()}\")\n",
    "    \n",
    "    return device, device_name\n",
    "\n",
    "device, device_name = detect_device()\n",
    "\n",
    "# Test device\n",
    "try:\n",
    "    test_tensor = torch.randn(10, 10).to(device)\n",
    "    result = torch.matmul(test_tensor, test_tensor.T)\n",
    "    del test_tensor, result\n",
    "except Exception as e:\n",
    "    print(f\"Device Test Failed: {e}\")\n",
    "    print(\"Falling back to CPU...\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0253782d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits:\n",
      "  train: 75,067 samples\n",
      "  test: 16,702 samples\n",
      "  validation: 16,433 samples\n",
      "  Total: 108,202 samples\n",
      "\n",
      "Sample data: {'speaker': 'fe016', 'text': 'okay.', 'basic_da': 'F', 'general_da': 'fg', 'full_da': 'fg'}\n",
      "Unique general_da labels: 12\n",
      "Labels: ['%', 'b', 'fg', 'fh', 'h', 'qh', 'qo', 'qr', 'qrr', 'qw', 'qy', 's']\n",
      "\n",
      "Label Distribution (Training Set):\n",
      "  %: 440 samples (2.7%)\n",
      "  b: 2,342 samples (14.3%)\n",
      "  fg: 527 samples (3.2%)\n",
      "  fh: 1,225 samples (7.5%)\n",
      "  h: 184 samples (1.1%)\n",
      "  qh: 36 samples (0.2%)\n",
      "  qo: 25 samples (0.2%)\n",
      "  qr: 39 samples (0.2%)\n",
      "  qrr: 73 samples (0.4%)\n",
      "  qw: 287 samples (1.7%)\n",
      "  qy: 806 samples (4.9%)\n",
      "  s: 10,449 samples (63.6%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"wylupek/mrda-corpus\")\n",
    "\n",
    "print(f\"Dataset splits:\")\n",
    "for split_name, split_data in dataset.items():\n",
    "    print(f\"  {split_name}: {len(split_data):,} samples\")\n",
    "total_samples = sum(len(split) for split in dataset.values())\n",
    "print(f\"  Total: {total_samples:,} samples\\n\")\n",
    "\n",
    "print(f\"Sample data: {dataset['train'][0]}\")\n",
    "\n",
    "\n",
    "train_labels = [sample['general_da'] for sample in dataset['train']]\n",
    "unique_labels = list(set(train_labels))\n",
    "unique_labels.sort()\n",
    "print(f\"Unique general_da labels: {len(unique_labels)}\")\n",
    "print(f\"Labels: {unique_labels}\\n\")\n",
    "\n",
    "\n",
    "label_counts = pd.Series(train_labels).value_counts().sort_index()\n",
    "print(f\"Label Distribution (Training Set):\")\n",
    "for label, count in label_counts.items():\n",
    "    percentage = (count / len(train_labels)) * 100\n",
    "    print(f\"  {label}: {count:,} samples ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a79a2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split:\n",
      "  Content: 54,123 samples (72.1%)\n",
      "  Non-content: 20,944 samples (27.9%)\n",
      "Validation split:\n",
      "  Content: 11,715 samples (71.3%)\n",
      "  Non-content: 4,718 samples (28.7%)\n",
      "Test split:\n",
      "  Content: 11,848 samples (70.9%)\n",
      "  Non-content: 4,854 samples (29.1%)\n"
     ]
    }
   ],
   "source": [
    "CONTENT_LABELS = {\n",
    "    's',      # Statement (65.2% - main content)\n",
    "    'qy',     # Yes-No-question (4.4%)\n",
    "    'qw',     # Wh-Question (1.5%)\n",
    "    'qh',     # Rhetorical Question (0.3%)\n",
    "    'qrr',    # Or-Clause (0.3%)\n",
    "    'qr',     # Or Question (0.2%)\n",
    "    'qo'      # Open-ended Question (0.2%)\n",
    "}\n",
    "\n",
    "NON_CONTENT_LABELS = {\n",
    "    'b',      # Continuer (14.1% - backchannels)\n",
    "    'fh',     # Floor Holder (7.5% - floor management)\n",
    "    'fg',     # Floor Grabber (2.8% - floor management)\n",
    "    '%',      # Interrupted/Abandoned (2.9% - disruptions)\n",
    "    'h'       # Hold Before Answer (0.6% - hesitations)\n",
    "}\n",
    "\n",
    "def calculate_content_distribution(labels):\n",
    "    \"\"\"Calculate content vs non-content percentages\"\"\"\n",
    "    content_count = sum(1 for label in labels if label in CONTENT_LABELS)\n",
    "    non_content_count = sum(1 for label in labels if label in NON_CONTENT_LABELS)\n",
    "    total = len(labels)\n",
    "    \n",
    "    content_pct = (content_count / total) * 100\n",
    "    non_content_pct = (non_content_count / total) * 100\n",
    "    \n",
    "    return content_count, non_content_count, content_pct, non_content_pct\n",
    "\n",
    "def map_to_binary(general_da_label):\n",
    "    \"\"\"Map general DA label to binary content/non-content\"\"\"\n",
    "    if general_da_label in CONTENT_LABELS:\n",
    "        return 1  # Content\n",
    "    elif general_da_label in NON_CONTENT_LABELS:\n",
    "        return 0  # Non-content\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label: {general_da_label}\")\n",
    "\n",
    "def map_to_text(general_da_label):\n",
    "    \"\"\"Map general DA label to text description\"\"\"\n",
    "    if general_da_label in CONTENT_LABELS:\n",
    "        return \"content\"\n",
    "    elif general_da_label in NON_CONTENT_LABELS:\n",
    "        return \"non-content\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label: {general_da_label}\")\n",
    "\n",
    "\n",
    "for split_name in ['train', 'validation', 'test']:\n",
    "    split_labels = [sample['general_da'] for sample in dataset[split_name]]\n",
    "    content_count, non_content_count, content_pct, non_content_pct = calculate_content_distribution(split_labels)\n",
    "    \n",
    "    print(f\"{split_name.capitalize()} split:\")\n",
    "    print(f\"  Content: {content_count:,} samples ({content_pct:.1f}%)\")\n",
    "    print(f\"  Non-content: {non_content_count:,} samples ({non_content_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "132962f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample text: 'okay so um i was going to try to get out of here'\n",
      "Tokenized shape: torch.Size([1, 15])\n",
      "Tokens: [101, 3100, 2061, 8529, 1045, 2001, 2183, 2000, 3046, 2000, 2131, 2041, 1997, 2182, 102]\n",
      "\n",
      "Logits shape: torch.Size([1, 12])\n",
      "Predictions: [0.08127684891223907, 0.08946936577558517, 0.08599119633436203, 0.07534374296665192, 0.08128724992275238, 0.08051715791225433, 0.08164877444505692, 0.07966802269220352, 0.09804148972034454, 0.08327770233154297, 0.07724126428365707, 0.08623714745044708]\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "NUM_LABELS = 12\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Test tokenization\n",
    "sample_text = \"okay so um i was going to try to get out of here\"\n",
    "encoded = tokenizer(sample_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(f\"\\nSample text: '{sample_text}'\")\n",
    "print(f\"Tokenized shape: {encoded['input_ids'].shape}\")\n",
    "print(f\"Tokens: {encoded['input_ids'][0].tolist()}\\n\")\n",
    "\n",
    "\n",
    "encoded = encoded.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Predictions: {predictions.tolist()[0]}\")\n",
    "\n",
    "del encoded, outputs, logits, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03248c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping:\n",
      "  % -> 0\n",
      "  b -> 1\n",
      "  fg -> 2\n",
      "  fh -> 3\n",
      "  h -> 4\n",
      "  qh -> 5\n",
      "  qo -> 6\n",
      "  qr -> 7\n",
      "  qrr -> 8\n",
      "  qw -> 9\n",
      "  qy -> 10\n",
      "  s -> 11\n",
      "\n",
      "Max length: 96\n"
     ]
    }
   ],
   "source": [
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(\"Label mapping:\")\n",
    "for label, idx in label2id.items():\n",
    "    print(f\"  {label} -> {idx}\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize text and encode labels\"\"\"\n",
    "    tokens = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,  # Will pad later in DataCollator\n",
    "        max_length=128,  # Actual max length is 96\n",
    "        return_tensors=None\n",
    "    )\n",
    "    tokens[\"labels\"] = [label2id[label] for label in examples[\"general_da\"]]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to all splits\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "# Verify no data loss\n",
    "if len(dataset[\"train\"]) != len(tokenized_datasets[\"train\"]):\n",
    "    print(f\"Not all samples were processed\\n\")\n",
    "    print(f\"Original train size: {len(dataset['train']):,}\")\n",
    "    print(f\"Processed train size: {len(tokenized_datasets['train']):,}\")\n",
    "if len(dataset[\"validation\"]) != len(tokenized_datasets[\"validation\"]):\n",
    "    print(f\"Not all samples were processed\\n\")\n",
    "    print(f\"Original validation size: {len(dataset['validation']):,}\")\n",
    "    print(f\"Processed validation size: {len(tokenized_datasets['validation']):,}\")\n",
    "if len(dataset[\"test\"]) != len(tokenized_datasets[\"test\"]):\n",
    "    print(f\"Not all samples were processed\\n\")\n",
    "    print(f\"Original test size: {len(dataset['test']):,}\")\n",
    "    print(f\"Processed test size: {len(tokenized_datasets['test']):,}\")\n",
    "\n",
    "# Check max length\n",
    "print(\"\\nMax length:\", max(max(len(s[\"input_ids\"]) for s in tokenized_datasets[split]) for split in [\"train\",\"validation\",\"test\"]))\n",
    "\n",
    "# Create data collator for batching\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b82e789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights (balanced method):\n",
      "  0 (%): 0.796 (count: 2171)\n",
      "  1 (b): 0.383 (count: 10606)\n",
      "  2 (fg): 0.820 (count: 2076)\n",
      "  3 (fh): 0.478 (count: 5617)\n",
      "  4 (h): 2.656 (count: 474)\n",
      "  5 (qh): 4.615 (count: 260)\n",
      "  6 (qo): 10.000 (count: 116)\n",
      "  7 (qr): 8.887 (count: 131)\n",
      "  8 (qrr): 4.899 (count: 244)\n",
      "  9 (qw): 1.293 (count: 1110)\n",
      "  10 (qy): 0.618 (count: 3310)\n",
      "  11 (s): 0.300 (count: 48952)\n"
     ]
    }
   ],
   "source": [
    "MAX_WEIGHT = 10\n",
    "MIN_WEIGHT = 0.3\n",
    "\n",
    "raw_train_labels = [sample[\"labels\"] for sample in tokenized_datasets[\"train\"]]\n",
    "total_train_examples = len(raw_train_labels)\n",
    "train_label_counts = dict(sorted(Counter(raw_train_labels).items(), key=lambda x: x[0]))\n",
    "\n",
    "# Compute weights\n",
    "class_weights = [1 / x for x in train_label_counts.values()]\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Scale weights to MAX_WEIGHT and MIN_WEIGHT\n",
    "curr_max_weight = class_weights_tensor.max()\n",
    "curr_min_weight = class_weights_tensor.min()\n",
    "class_weights_tensor = (class_weights_tensor - curr_min_weight) / (curr_max_weight - curr_min_weight) * (MAX_WEIGHT - MIN_WEIGHT) + MIN_WEIGHT\n",
    "\n",
    "print(f\"Class weights (balanced method):\")\n",
    "for class_id in range(NUM_LABELS):\n",
    "    print(f\"  {class_id} ({id2label[class_id]}): {class_weights_tensor[class_id].item() :.3f} (count: {train_label_counts[class_id]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bce13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal loss with balanced cross entropy\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Custom Weighted Trainer\n",
    "class AdvancedTrainer(Trainer):\n",
    "    def __init__(self, loss_type=\"focal_weighted\", class_weights=None, focal_gamma=2.0, loss_reduction=\"mean\", *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_type = loss_type\n",
    "        self.class_weights = class_weights\n",
    "        \n",
    "        if loss_type == \"focal_weighted\":\n",
    "            self.loss_fn = FocalLoss(alpha=class_weights, gamma=focal_gamma, reduction=loss_reduction)\n",
    "        elif loss_type == \"focal\":\n",
    "            self.loss_fn = FocalLoss(gamma=focal_gamma, reduction=loss_reduction)\n",
    "        elif loss_type == \"ce_weighted\":\n",
    "            self.loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        elif loss_type == \"ce\":\n",
    "            self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        loss = self.loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
    "    micro_f1 = f1_score(labels, predictions, average='micro') # NOTE Accuracy is the same as micro F1\n",
    "    weighted_f1 = f1_score(labels, predictions, average='weighted')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"weighted_f1\": weighted_f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff815852",
   "metadata": {},
   "source": [
    "### Parameters tuning\n",
    "**LoRA**\n",
    "- lora_dropout – Regular dropout regularization, prevents overfitting. (0.0; 0.3)\n",
    "- lora_r – The size of LoRA adapters, means how much new information the model can learn. High values cause better learning capacity, especially usefull for imbalanced data  (8; 64)\n",
    "- lora_alpha – How strong the LoRA adapters influence the original model. Affects effective scaling (`lora_alpha/lora_r`) (scale between 0.5 and 8)\n",
    "\n",
    "**Traning arguments**\n",
    "\n",
    "NOTE: Step is calculated by `ceil(total_samples / batch_size)`\n",
    "\n",
    "Proper Warmup Guidelines:\n",
    "- Simple tasks: 5-10% of total steps\n",
    "- Complex tasks: 10-15% of total steps\n",
    "- Imbalanced/Difficult: 15-20% of total steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2e9e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 14076\n",
      "Steps per epoch: 2346\n",
      "Steps per eval: 391\n",
      "Steps per logging: 195\n",
      "Steps per saving: 1173\n"
     ]
    }
   ],
   "source": [
    "LORA_DROPOUT = 0.15\n",
    "LORA_R = 64 # TO RUN\n",
    "LORA_ALPHA = 128\n",
    "TARGET_MODULES = [\"q_lin\", \"v_lin\", \"k_lin\", \"out_lin\"]\n",
    "\n",
    "NO_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.00008\n",
    "WARMUP_STEPS = 0.15\n",
    "WEIGHT_DECAY = 0.05\n",
    "\n",
    "LOSS_TYPE = \"focal_weighted\"\n",
    "FOCAL_GAMMA=2.0\n",
    "LOSS_REDUCTION=\"mean\"\n",
    "\n",
    "### LoRA setup ###\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    # modules_to_save=[\"pre_classifier\", \"classifier\"],  # Keep classification head trainable! -- unfrozen by default\n",
    "    ### TUNABLE PARAMETERS ###\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    ")\n",
    "advanced_peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# # Check which parameters are trainable\n",
    "# print(\"\\n=== Trainable Parameters Check ===\")\n",
    "# for name, param in advanced_peft_model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(f\"✅ {name}: {param.shape}\")\n",
    "#     elif \"classifier\" in name or \"pre_classifier\" in name:\n",
    "#         print(f\"❌ FROZEN: {name}: {param.shape}\")\n",
    "\n",
    "# # Print summary\n",
    "# print(f\"\\nTotal trainable parameters: {sum(p.numel() for p in advanced_peft_model.parameters() if p.requires_grad):,}\")\n",
    "# print(f\"Classifier head status: {'✅ TRAINABLE' if any('classifier' in name for name, p in advanced_peft_model.named_parameters() if p.requires_grad) else '❌ FROZEN'}\")\n",
    "\n",
    "### Training setup ###\n",
    "total_steps = NO_EPOCHS * np.ceil(len(tokenized_datasets[\"train\"]) / BATCH_SIZE)\n",
    "eval_steps = total_steps // NO_EPOCHS // 6 # 6 times per epoch\n",
    "save_steps = eval_steps * 3 # 2 times per epoch\n",
    "logging_steps = total_steps // NO_EPOCHS // 12 # 12 times per epoch\n",
    "advanced_training_args = TrainingArguments(\n",
    "    output_dir=\"./advanced_checkpoints\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",  # Optimize for macro F1, perfect for imbalanced data\n",
    "    greater_is_better=True,\n",
    "    report_to=None,\n",
    "    remove_unused_columns=False, # Required for custom trainer\n",
    "    dataloader_num_workers=0,  # Important for MPS compatibility\n",
    "    eval_steps=eval_steps,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    ### TUNABLE PARAMETERS ###\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NO_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=int(WARMUP_STEPS * total_steps), # Prevents overfitting, should be ~15% of total steps for imbalanced data\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "advanced_trainer = AdvancedTrainer(\n",
    "    loss_type=LOSS_TYPE,  # Focal loss for imbalanced data\n",
    "    class_weights=class_weights_tensor, # Weights for imbalanced data\n",
    "    model=advanced_peft_model,\n",
    "    args=advanced_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    ### TUNABLE PARAMETERS ###\n",
    "    focal_gamma=FOCAL_GAMMA,\n",
    "    loss_reduction=LOSS_REDUCTION,\n",
    ")\n",
    "\n",
    "print(f\"Total steps: {int(total_steps)}\")\n",
    "print(f\"Steps per epoch: {int(total_steps // NO_EPOCHS)}\")\n",
    "print(f\"Steps per eval: {int(eval_steps)}\")\n",
    "print(f\"Steps per logging: {int(logging_steps)}\")\n",
    "print(f\"Steps per saving: {int(save_steps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d3c93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting advanced training with:\n",
      "{'max_weight': 10, 'min_weight': 0.3, 'lora_dropout': 0.15, 'lora_r': 32, 'lora_alpha': 64, 'target_modules': ['q_lin', 'v_lin', 'k_lin', 'out_lin', 'lin1', 'lin2'], 'loss_type': 'focal', 'no_epochs': 6, 'batch_size': 32, 'learning_rate': 0.0001, 'warmup_steps': 0.15, 'weight_decay': 0.04, 'focal_gamma': 2.0, 'loss_reduction': 'mean'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14076' max='14076' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14076/14076 22:11, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>0.440500</td>\n",
       "      <td>0.448869</td>\n",
       "      <td>0.754032</td>\n",
       "      <td>0.264415</td>\n",
       "      <td>0.754032</td>\n",
       "      <td>0.731951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>0.299000</td>\n",
       "      <td>0.298252</td>\n",
       "      <td>0.761395</td>\n",
       "      <td>0.426321</td>\n",
       "      <td>0.761395</td>\n",
       "      <td>0.767846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1173</td>\n",
       "      <td>0.194900</td>\n",
       "      <td>0.256027</td>\n",
       "      <td>0.766811</td>\n",
       "      <td>0.450354</td>\n",
       "      <td>0.766811</td>\n",
       "      <td>0.779256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1564</td>\n",
       "      <td>0.217400</td>\n",
       "      <td>0.207176</td>\n",
       "      <td>0.778920</td>\n",
       "      <td>0.505271</td>\n",
       "      <td>0.778920</td>\n",
       "      <td>0.787224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1955</td>\n",
       "      <td>0.191800</td>\n",
       "      <td>0.193147</td>\n",
       "      <td>0.760665</td>\n",
       "      <td>0.515588</td>\n",
       "      <td>0.760665</td>\n",
       "      <td>0.775542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2346</td>\n",
       "      <td>0.192100</td>\n",
       "      <td>0.193797</td>\n",
       "      <td>0.737784</td>\n",
       "      <td>0.510712</td>\n",
       "      <td>0.737784</td>\n",
       "      <td>0.757816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2737</td>\n",
       "      <td>0.188400</td>\n",
       "      <td>0.197360</td>\n",
       "      <td>0.749954</td>\n",
       "      <td>0.546124</td>\n",
       "      <td>0.749954</td>\n",
       "      <td>0.770279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3128</td>\n",
       "      <td>0.173000</td>\n",
       "      <td>0.199541</td>\n",
       "      <td>0.759326</td>\n",
       "      <td>0.512585</td>\n",
       "      <td>0.759326</td>\n",
       "      <td>0.777534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3519</td>\n",
       "      <td>0.185600</td>\n",
       "      <td>0.188554</td>\n",
       "      <td>0.778129</td>\n",
       "      <td>0.558562</td>\n",
       "      <td>0.778129</td>\n",
       "      <td>0.790481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3910</td>\n",
       "      <td>0.175500</td>\n",
       "      <td>0.186740</td>\n",
       "      <td>0.729995</td>\n",
       "      <td>0.532319</td>\n",
       "      <td>0.729995</td>\n",
       "      <td>0.758123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4301</td>\n",
       "      <td>0.142800</td>\n",
       "      <td>0.191137</td>\n",
       "      <td>0.734072</td>\n",
       "      <td>0.554492</td>\n",
       "      <td>0.734072</td>\n",
       "      <td>0.766065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4692</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.202366</td>\n",
       "      <td>0.769306</td>\n",
       "      <td>0.535333</td>\n",
       "      <td>0.769306</td>\n",
       "      <td>0.782292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5083</td>\n",
       "      <td>0.141700</td>\n",
       "      <td>0.205618</td>\n",
       "      <td>0.770949</td>\n",
       "      <td>0.567752</td>\n",
       "      <td>0.770949</td>\n",
       "      <td>0.785793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5474</td>\n",
       "      <td>0.109500</td>\n",
       "      <td>0.226732</td>\n",
       "      <td>0.764255</td>\n",
       "      <td>0.547322</td>\n",
       "      <td>0.764255</td>\n",
       "      <td>0.779044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5865</td>\n",
       "      <td>0.134600</td>\n",
       "      <td>0.209342</td>\n",
       "      <td>0.755735</td>\n",
       "      <td>0.537999</td>\n",
       "      <td>0.755735</td>\n",
       "      <td>0.777256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6256</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>0.213622</td>\n",
       "      <td>0.761516</td>\n",
       "      <td>0.535383</td>\n",
       "      <td>0.761516</td>\n",
       "      <td>0.777315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6647</td>\n",
       "      <td>0.138800</td>\n",
       "      <td>0.212523</td>\n",
       "      <td>0.752814</td>\n",
       "      <td>0.550432</td>\n",
       "      <td>0.752814</td>\n",
       "      <td>0.771151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7038</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>0.189422</td>\n",
       "      <td>0.771922</td>\n",
       "      <td>0.566259</td>\n",
       "      <td>0.771922</td>\n",
       "      <td>0.786928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7429</td>\n",
       "      <td>0.118600</td>\n",
       "      <td>0.202736</td>\n",
       "      <td>0.779590</td>\n",
       "      <td>0.553948</td>\n",
       "      <td>0.779590</td>\n",
       "      <td>0.793044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7820</td>\n",
       "      <td>0.140800</td>\n",
       "      <td>0.193828</td>\n",
       "      <td>0.737236</td>\n",
       "      <td>0.553035</td>\n",
       "      <td>0.737236</td>\n",
       "      <td>0.768082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8211</td>\n",
       "      <td>0.107600</td>\n",
       "      <td>0.205657</td>\n",
       "      <td>0.778555</td>\n",
       "      <td>0.571430</td>\n",
       "      <td>0.778555</td>\n",
       "      <td>0.791319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8602</td>\n",
       "      <td>0.136600</td>\n",
       "      <td>0.195233</td>\n",
       "      <td>0.741009</td>\n",
       "      <td>0.547329</td>\n",
       "      <td>0.741009</td>\n",
       "      <td>0.764691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8993</td>\n",
       "      <td>0.133500</td>\n",
       "      <td>0.200446</td>\n",
       "      <td>0.771010</td>\n",
       "      <td>0.562250</td>\n",
       "      <td>0.771010</td>\n",
       "      <td>0.786392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9384</td>\n",
       "      <td>0.140600</td>\n",
       "      <td>0.198651</td>\n",
       "      <td>0.772531</td>\n",
       "      <td>0.538324</td>\n",
       "      <td>0.772531</td>\n",
       "      <td>0.788665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9775</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.194850</td>\n",
       "      <td>0.742895</td>\n",
       "      <td>0.543228</td>\n",
       "      <td>0.742895</td>\n",
       "      <td>0.769512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10166</td>\n",
       "      <td>0.107500</td>\n",
       "      <td>0.205129</td>\n",
       "      <td>0.778616</td>\n",
       "      <td>0.568180</td>\n",
       "      <td>0.778616</td>\n",
       "      <td>0.792874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10557</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>0.201730</td>\n",
       "      <td>0.759752</td>\n",
       "      <td>0.555462</td>\n",
       "      <td>0.759752</td>\n",
       "      <td>0.780405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10948</td>\n",
       "      <td>0.103100</td>\n",
       "      <td>0.210620</td>\n",
       "      <td>0.746668</td>\n",
       "      <td>0.563973</td>\n",
       "      <td>0.746668</td>\n",
       "      <td>0.774728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11339</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>0.204319</td>\n",
       "      <td>0.744356</td>\n",
       "      <td>0.558617</td>\n",
       "      <td>0.744356</td>\n",
       "      <td>0.772459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11730</td>\n",
       "      <td>0.123600</td>\n",
       "      <td>0.210551</td>\n",
       "      <td>0.775634</td>\n",
       "      <td>0.567551</td>\n",
       "      <td>0.775634</td>\n",
       "      <td>0.789820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12121</td>\n",
       "      <td>0.101000</td>\n",
       "      <td>0.214362</td>\n",
       "      <td>0.771618</td>\n",
       "      <td>0.562551</td>\n",
       "      <td>0.771618</td>\n",
       "      <td>0.787308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12512</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>0.212742</td>\n",
       "      <td>0.760421</td>\n",
       "      <td>0.547120</td>\n",
       "      <td>0.760421</td>\n",
       "      <td>0.779394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12903</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>0.211395</td>\n",
       "      <td>0.763403</td>\n",
       "      <td>0.549697</td>\n",
       "      <td>0.763403</td>\n",
       "      <td>0.782514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13294</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.216669</td>\n",
       "      <td>0.772227</td>\n",
       "      <td>0.557920</td>\n",
       "      <td>0.772227</td>\n",
       "      <td>0.788794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13685</td>\n",
       "      <td>0.111200</td>\n",
       "      <td>0.215181</td>\n",
       "      <td>0.765959</td>\n",
       "      <td>0.555226</td>\n",
       "      <td>0.765959</td>\n",
       "      <td>0.783769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14076</td>\n",
       "      <td>0.099300</td>\n",
       "      <td>0.214749</td>\n",
       "      <td>0.765107</td>\n",
       "      <td>0.555100</td>\n",
       "      <td>0.765107</td>\n",
       "      <td>0.783918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advanced training completed!\n",
      "Final train loss: 0.1557\n"
     ]
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    \"max_weight\": MAX_WEIGHT,\n",
    "    \"min_weight\": MIN_WEIGHT,\n",
    "\n",
    "    \"lora_dropout\": LORA_DROPOUT,\n",
    "    \"lora_r\": LORA_R,\n",
    "    \"lora_alpha\": LORA_ALPHA,\n",
    "    \"target_modules\": TARGET_MODULES,\n",
    "\n",
    "    \"no_epochs\": NO_EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"warmup_steps\": WARMUP_STEPS,\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "\n",
    "    \"loss_type\": LOSS_TYPE,\n",
    "    \"focal_gamma\": FOCAL_GAMMA,\n",
    "    \"loss_reduction\": LOSS_REDUCTION,\n",
    "}\n",
    "\n",
    "print(f\"\\nStarting advanced training with:\")\n",
    "print(hyperparams)\n",
    "\n",
    "# Run advanced training\n",
    "advanced_result = advanced_trainer.train()\n",
    "\n",
    "print(f\"\\nAdvanced training completed!\")\n",
    "print(f\"Final train loss: {advanced_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b11942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PER-CLASS CONFUSION ANALYSIS (12 TABLES)\n",
      "================================================================================\n",
      "\n",
      "📊 CLASS 0: '%' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP= 337 | FP= 534\n",
      "                FN= 103 | TN=15459\n",
      "  Metrics:      Prec=0.387 | Rec=0.766 | F1=0.514\n",
      "  Distribution: True=  2.7% | Pred=  5.3% | Diff= +2.6%\n",
      "\n",
      "📊 CLASS 1: 'b' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP=2102 | FP=1111\n",
      "                FN= 240 | TN=12980\n",
      "  Metrics:      Prec=0.654 | Rec=0.898 | F1=0.757\n",
      "  Distribution: True= 14.3% | Pred= 19.6% | Diff= +5.3%\n",
      "\n",
      "📊 CLASS 2: 'fg' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP= 148 | FP= 484\n",
      "                FN= 379 | TN=15422\n",
      "  Metrics:      Prec=0.234 | Rec=0.281 | F1=0.255\n",
      "  Distribution: True=  3.2% | Pred=  3.8% | Diff= +0.6%\n",
      "\n",
      "📊 CLASS 3: 'fh' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP= 703 | FP= 412\n",
      "                FN= 522 | TN=14796\n",
      "  Metrics:      Prec=0.630 | Rec=0.574 | F1=0.601\n",
      "  Distribution: True=  7.5% | Pred=  6.8% | Diff= -0.7%\n",
      "\n",
      "📊 CLASS 4: 'h' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP=  76 | FP= 285\n",
      "                FN= 108 | TN=15964\n",
      "  Metrics:      Prec=0.211 | Rec=0.413 | F1=0.279\n",
      "  Distribution: True=  1.1% | Pred=  2.2% | Diff= +1.1%\n",
      "\n",
      "📊 CLASS 5: 'qh' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP=  17 | FP=  67\n",
      "                FN=  19 | TN=16330\n",
      "  Metrics:      Prec=0.202 | Rec=0.472 | F1=0.283\n",
      "  Distribution: True=  0.2% | Pred=  0.5% | Diff= +0.3%\n",
      "\n",
      "📊 CLASS 6: 'qo' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP=   8 | FP=  16\n",
      "                FN=  17 | TN=16392\n",
      "  Metrics:      Prec=0.333 | Rec=0.320 | F1=0.327\n",
      "  Distribution: True=  0.2% | Pred=  0.1% | Diff= -0.0%\n",
      "\n",
      "📊 CLASS 7: 'qr' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP=  28 | FP=  38\n",
      "                FN=  11 | TN=16356\n",
      "  Metrics:      Prec=0.424 | Rec=0.718 | F1=0.533\n",
      "  Distribution: True=  0.2% | Pred=  0.4% | Diff= +0.2%\n",
      "\n",
      "📊 CLASS 8: 'qrr' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP=  68 | FP=  37\n",
      "                FN=   5 | TN=16323\n",
      "  Metrics:      Prec=0.648 | Rec=0.932 | F1=0.764\n",
      "  Distribution: True=  0.4% | Pred=  0.6% | Diff= +0.2%\n",
      "\n",
      "📊 CLASS 9: 'qw' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP= 206 | FP=  41\n",
      "                FN=  81 | TN=16105\n",
      "  Metrics:      Prec=0.834 | Rec=0.718 | F1=0.772\n",
      "  Distribution: True=  1.7% | Pred=  1.5% | Diff= -0.2%\n",
      "\n",
      "📊 CLASS 10: 'qy' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP= 732 | FP=  75\n",
      "                FN=  74 | TN=15552\n",
      "  Metrics:      Prec=0.907 | Rec=0.908 | F1=0.908\n",
      "  Distribution: True=  4.9% | Pred=  4.9% | Diff= +0.0%\n",
      "\n",
      "📊 CLASS 11: 's' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP=8369 | FP= 539\n",
      "                FN=2080 | TN=5445\n",
      "  Metrics:      Prec=0.939 | Rec=0.801 | F1=0.865\n",
      "  Distribution: True= 63.6% | Pred= 54.2% | Diff= -9.4%\n",
      "\n",
      "🎯 OVERALL: Acc=0.779 | MacroF1=0.571 | WeightedF1=0.791\n",
      "💾 Training progress saved with 36 checkpoints\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT LOGGING & COMPREHENSIVE EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "def setup_experiment_logging(experiment_name, hyperparams):\n",
    "    \"\"\"Setup timestamped logging directory for experiments\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_dir = f\"results/{experiment_name}_{timestamp}\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Save hyperparameters for reproducibility\n",
    "    with open(f\"{results_dir}/hyperparams.json\", 'w') as f:\n",
    "        json.dump(hyperparams, f, indent=2)\n",
    "    \n",
    "    return results_dir\n",
    "\n",
    "\n",
    "def comprehensive_evaluation(trainer, tokenized_datasets, id2label, results_dir):\n",
    "    \"\"\"\n",
    "    Complete evaluation with:\n",
    "    - Overall metrics (accuracy, macro/weighted F1)\n",
    "    - Per-class metrics and confusion matrix\n",
    "    - Training progress table with all checkpoints\n",
    "    - Detailed 12-class analysis with TP/FP/FN/TN breakdown\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===== GET PREDICTIONS =====\n",
    "    predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "    y_true = predictions.label_ids\n",
    "    y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "    \n",
    "    # ===== OVERALL METRICS =====\n",
    "    metrics = {\n",
    "        'accuracy': round(accuracy_score(y_true, y_pred), 4),\n",
    "        'macro_f1': round(f1_score(y_true, y_pred, average='macro'), 4),\n",
    "        'weighted_f1': round(f1_score(y_true, y_pred, average='weighted'), 4),\n",
    "    }\n",
    "    \n",
    "    # ===== PER-CLASS METRICS =====\n",
    "    class_report = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=[id2label[i] for i in range(len(id2label))],\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    def round_nested_dict(d, decimals=4):\n",
    "        \"\"\"Recursively round all numeric values in nested dictionary\"\"\"\n",
    "        if isinstance(d, dict):\n",
    "            return {k: round_nested_dict(v, decimals) for k, v in d.items()}\n",
    "        elif isinstance(d, (int, float)):\n",
    "            return round(d, decimals) if isinstance(d, float) else d\n",
    "        else:\n",
    "            return d\n",
    "    \n",
    "    class_report = round_nested_dict(class_report)\n",
    "    \n",
    "    # ===== CONFUSION MATRIX =====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # ===== TRAINING PROGRESS EXTRACTION =====\n",
    "    training_progress = []\n",
    "    if hasattr(trainer, 'state') and hasattr(trainer.state, 'log_history'):\n",
    "        # Separate evaluation and training logs\n",
    "        eval_logs = [log for log in trainer.state.log_history if 'eval_loss' in log]\n",
    "        train_logs = [log for log in trainer.state.log_history if 'loss' in log and 'eval_loss' not in log]\n",
    "        \n",
    "        # Map training steps to training loss\n",
    "        train_loss_map = {}\n",
    "        for log in train_logs:\n",
    "            step = log.get('step', 0)\n",
    "            if step > 0:\n",
    "                train_loss_map[step] = log.get('loss', 0)\n",
    "        \n",
    "        # Match evaluation checkpoints with training loss\n",
    "        for log in eval_logs:\n",
    "            eval_step = log.get('step', 0)\n",
    "            \n",
    "            # Find closest training loss (≤ eval_step)\n",
    "            training_loss = 0\n",
    "            if train_loss_map:\n",
    "                valid_train_steps = [s for s in train_loss_map.keys() if s <= eval_step]\n",
    "                if valid_train_steps:\n",
    "                    closest_train_step = max(valid_train_steps)\n",
    "                    training_loss = train_loss_map[closest_train_step]\n",
    "            \n",
    "            progress_entry = {\n",
    "                'step': eval_step,\n",
    "                'training_loss': round(training_loss, 4),\n",
    "                'validation_loss': round(log.get('eval_loss', 0), 4),\n",
    "                'accuracy': round(log.get('eval_accuracy', 0), 4),\n",
    "                'macro_f1': round(log.get('eval_macro_f1', 0), 4),\n",
    "                'micro_f1': round(log.get('eval_micro_f1', 0), 4),\n",
    "                'weighted_f1': round(log.get('eval_weighted_f1', 0), 4)\n",
    "            }\n",
    "            training_progress.append(progress_entry)\n",
    "    \n",
    "    # ===== DETAILED 12-CLASS ANALYSIS =====\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PER-CLASS CONFUSION ANALYSIS (12 TABLES)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    detailed_analysis = {}\n",
    "    for class_id in range(len(id2label)):\n",
    "        class_name = id2label[class_id]\n",
    "        \n",
    "        # Calculate TP/FP/FN/TN for this class\n",
    "        tp = cm[class_id, class_id]\n",
    "        fp = cm[:, class_id].sum() - tp\n",
    "        fn = cm[class_id, :].sum() - tp\n",
    "        tn = cm.sum() - tp - fp - fn\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # Calculate distribution percentages\n",
    "        true_count = (y_true == class_id).sum()\n",
    "        pred_count = (y_pred == class_id).sum()\n",
    "        true_pct = (true_count / len(y_true)) * 100\n",
    "        pred_pct = (pred_count / len(y_pred)) * 100\n",
    "        \n",
    "        # Store detailed analysis\n",
    "        detailed_analysis[class_name] = {\n",
    "            'tp': int(tp), 'fp': int(fp), 'fn': int(fn), 'tn': int(tn),\n",
    "            'precision': round(precision, 4), 'recall': round(recall, 4), 'f1': round(f1, 4),\n",
    "            'true_pct': round(true_pct, 4), 'pred_pct': round(pred_pct, 4), \n",
    "            'diff_pct': round(pred_pct - true_pct, 4)\n",
    "        }\n",
    "        \n",
    "        # Print formatted class analysis\n",
    "        print(f\"\\n📊 CLASS {class_id}: '{class_name}' Analysis\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"  Confusion:    TP={tp:4d} | FP={fp:4d}\")\n",
    "        print(f\"                FN={fn:4d} | TN={tn:4d}\")\n",
    "        print(f\"  Metrics:      Prec={precision:.3f} | Rec={recall:.3f} | F1={f1:.3f}\")\n",
    "        print(f\"  Distribution: True={true_pct:5.1f}% | Pred={pred_pct:5.1f}% | Diff={pred_pct-true_pct:+5.1f}%\")\n",
    "    \n",
    "    # ===== CLASS DISTRIBUTION ANALYSIS =====\n",
    "    true_dist = pd.Series(y_true).value_counts().sort_index()\n",
    "    pred_dist = pd.Series(y_pred).value_counts().sort_index()\n",
    "    \n",
    "    # ===== SAVE COMPREHENSIVE RESULTS =====\n",
    "    results = {\n",
    "        'overall_metrics': metrics,\n",
    "        'per_class_metrics': class_report,\n",
    "        'detailed_class_analysis': detailed_analysis,\n",
    "        'training_progress': training_progress,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'true_distribution': true_dist.to_dict(),\n",
    "        'pred_distribution': pred_dist.to_dict()\n",
    "    }\n",
    "    \n",
    "    with open(f\"{results_dir}/evaluation.json\", 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # ===== SUMMARY OUTPUT =====\n",
    "    print(f\"\\n🎯 OVERALL: Acc={metrics['accuracy']:.3f} | MacroF1={metrics['macro_f1']:.3f} | WeightedF1={metrics['weighted_f1']:.3f}\")\n",
    "    print(f\"💾 Training progress saved with {len(training_progress)} checkpoints\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RUN COMPREHENSIVE EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "results_dir = setup_experiment_logging(\"no_1\", hyperparams)\n",
    "results = comprehensive_evaluation(advanced_trainer, tokenized_datasets, id2label, results_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
