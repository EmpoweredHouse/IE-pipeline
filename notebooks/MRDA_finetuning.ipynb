{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be9add11",
   "metadata": {},
   "source": [
    "# MRDA Dialogue Act Classification Pipeline\n",
    "\n",
    "## Multi-Stage Training Approach:\n",
    "1. **Stage 1:** Train 12-class General DA classifier \n",
    "2. **Stage 2:** Map to binary content/non-content classification\n",
    "\n",
    "**Target Model Repository:** `wylupek/distilbert-mrda-dialogue-acts`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0b6267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filip/Documents/github/IE-pipeline/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import platform\n",
    "import psutil\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from huggingface_hub import login, whoami\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d95a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: Linux 5.15.0-151-generic\n",
      "Architecture: x86_64\n",
      "CPU Cores: 24\n",
      "RAM: 31.2 GB\n",
      "CUDA Device: NVIDIA GeForce RTX 3060 Ti\n",
      "GPU Memory: 7.8 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"Architecture: {platform.machine()}\")\n",
    "print(f\"CPU Cores: {psutil.cpu_count()}\")\n",
    "print(f\"RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "\n",
    "\n",
    "def detect_device():\n",
    "    \"\"\"Detect best available device with fallback strategy\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        device_name = torch.cuda.get_device_name(0)\n",
    "        memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        print(f\"CUDA Device: {device_name}\")\n",
    "        print(f\"GPU Memory: {memory_gb:.1f} GB\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps\" \n",
    "        device_name = \"Apple Silicon (MPS)\"\n",
    "        print(f\"MPS Device: {device_name}\")\n",
    "        print(f\"Unified Memory Available\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        device_name = \"CPU\"\n",
    "        print(f\"CPU Device: {device_name}\")\n",
    "        print(f\"Using CPU cores: {psutil.cpu_count()}\")\n",
    "    \n",
    "    return device, device_name\n",
    "\n",
    "device, device_name = detect_device()\n",
    "\n",
    "# Test device\n",
    "try:\n",
    "    test_tensor = torch.randn(10, 10).to(device)\n",
    "    result = torch.matmul(test_tensor, test_tensor.T)\n",
    "    del test_tensor, result\n",
    "except Exception as e:\n",
    "    print(f\"Device Test Failed: {e}\")\n",
    "    print(\"Falling back to CPU...\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0253782d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits:\n",
      "  train: 75,067 samples\n",
      "  test: 16,702 samples\n",
      "  validation: 16,433 samples\n",
      "  Total: 108,202 samples\n",
      "\n",
      "Sample data: {'speaker': 'fe016', 'text': 'okay.', 'basic_da': 'F', 'general_da': 'fg', 'full_da': 'fg'}\n",
      "Unique general_da labels: 12\n",
      "Labels: ['%', 'b', 'fg', 'fh', 'h', 'qh', 'qo', 'qr', 'qrr', 'qw', 'qy', 's']\n",
      "\n",
      "Label Distribution (Training Set):\n",
      "  %: 440 samples (2.7%)\n",
      "  b: 2,342 samples (14.3%)\n",
      "  fg: 527 samples (3.2%)\n",
      "  fh: 1,225 samples (7.5%)\n",
      "  h: 184 samples (1.1%)\n",
      "  qh: 36 samples (0.2%)\n",
      "  qo: 25 samples (0.2%)\n",
      "  qr: 39 samples (0.2%)\n",
      "  qrr: 73 samples (0.4%)\n",
      "  qw: 287 samples (1.7%)\n",
      "  qy: 806 samples (4.9%)\n",
      "  s: 10,449 samples (63.6%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"wylupek/mrda-corpus\")\n",
    "\n",
    "print(f\"Dataset splits:\")\n",
    "for split_name, split_data in dataset.items():\n",
    "    print(f\"  {split_name}: {len(split_data):,} samples\")\n",
    "total_samples = sum(len(split) for split in dataset.values())\n",
    "print(f\"  Total: {total_samples:,} samples\\n\")\n",
    "\n",
    "print(f\"Sample data: {dataset['train'][0]}\")\n",
    "\n",
    "\n",
    "train_labels = [sample['general_da'] for sample in dataset['validation']]\n",
    "unique_labels = list(set(train_labels))\n",
    "unique_labels.sort()\n",
    "print(f\"Unique general_da labels: {len(unique_labels)}\")\n",
    "print(f\"Labels: {unique_labels}\\n\")\n",
    "\n",
    "\n",
    "label_counts = pd.Series(train_labels).value_counts().sort_index()\n",
    "print(f\"Label Distribution (Training Set):\")\n",
    "for label, count in label_counts.items():\n",
    "    percentage = (count / len(train_labels)) * 100\n",
    "    print(f\"  {label}: {count:,} samples ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a79a2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split:\n",
      "  Content: 54,123 samples (72.1%)\n",
      "  Non-content: 20,944 samples (27.9%)\n",
      "Validation split:\n",
      "  Content: 11,715 samples (71.3%)\n",
      "  Non-content: 4,718 samples (28.7%)\n",
      "Test split:\n",
      "  Content: 11,848 samples (70.9%)\n",
      "  Non-content: 4,854 samples (29.1%)\n"
     ]
    }
   ],
   "source": [
    "CONTENT_LABELS = {\n",
    "    's',      # Statement (65.2% - main content)\n",
    "    'qy',     # Yes-No-question (4.4%)\n",
    "    'qw',     # Wh-Question (1.5%)\n",
    "    'qh',     # Rhetorical Question (0.3%)\n",
    "    'qrr',    # Or-Clause (0.3%)\n",
    "    'qr',     # Or Question (0.2%)\n",
    "    'qo'      # Open-ended Question (0.2%)\n",
    "}\n",
    "\n",
    "NON_CONTENT_LABELS = {\n",
    "    'b',      # Continuer (14.1% - backchannels)\n",
    "    'fh',     # Floor Holder (7.5% - floor management)\n",
    "    'fg',     # Floor Grabber (2.8% - floor management)\n",
    "    '%',      # Interrupted/Abandoned (2.9% - disruptions)\n",
    "    'h'       # Hold Before Answer (0.6% - hesitations)\n",
    "}\n",
    "\n",
    "def calculate_content_distribution(labels):\n",
    "    \"\"\"Calculate content vs non-content percentages\"\"\"\n",
    "    content_count = sum(1 for label in labels if label in CONTENT_LABELS)\n",
    "    non_content_count = sum(1 for label in labels if label in NON_CONTENT_LABELS)\n",
    "    total = len(labels)\n",
    "    \n",
    "    content_pct = (content_count / total) * 100\n",
    "    non_content_pct = (non_content_count / total) * 100\n",
    "    \n",
    "    return content_count, non_content_count, content_pct, non_content_pct\n",
    "\n",
    "def map_to_binary(general_da_label):\n",
    "    \"\"\"Map general DA label to binary content/non-content\"\"\"\n",
    "    if general_da_label in CONTENT_LABELS:\n",
    "        return 1  # Content\n",
    "    elif general_da_label in NON_CONTENT_LABELS:\n",
    "        return 0  # Non-content\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label: {general_da_label}\")\n",
    "\n",
    "def map_to_text(general_da_label):\n",
    "    \"\"\"Map general DA label to text description\"\"\"\n",
    "    if general_da_label in CONTENT_LABELS:\n",
    "        return \"content\"\n",
    "    elif general_da_label in NON_CONTENT_LABELS:\n",
    "        return \"non-content\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label: {general_da_label}\")\n",
    "\n",
    "\n",
    "for split_name in ['train', 'validation', 'test']:\n",
    "    split_labels = [sample['general_da'] for sample in dataset[split_name]]\n",
    "    content_count, non_content_count, content_pct, non_content_pct = calculate_content_distribution(split_labels)\n",
    "    \n",
    "    print(f\"{split_name.capitalize()} split:\")\n",
    "    print(f\"  Content: {content_count:,} samples ({content_pct:.1f}%)\")\n",
    "    print(f\"  Non-content: {non_content_count:,} samples ({non_content_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "132962f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample text: 'okay so um i was going to try to get out of here'\n",
      "Tokenized shape: torch.Size([1, 15])\n",
      "Tokens: [101, 3100, 2061, 8529, 1045, 2001, 2183, 2000, 3046, 2000, 2131, 2041, 1997, 2182, 102]\n",
      "\n",
      "Logits shape: torch.Size([1, 12])\n",
      "Predictions: [0.08207350224256516, 0.08157573640346527, 0.07472242414951324, 0.08132435381412506, 0.07871118187904358, 0.09153793007135391, 0.07096831500530243, 0.09276984632015228, 0.08914028108119965, 0.08809887617826462, 0.07908013463020325, 0.08999744057655334]\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "NUM_LABELS = 12\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Test tokenization\n",
    "sample_text = \"okay so um i was going to try to get out of here\"\n",
    "encoded = tokenizer(sample_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(f\"\\nSample text: '{sample_text}'\")\n",
    "print(f\"Tokenized shape: {encoded['input_ids'].shape}\")\n",
    "print(f\"Tokens: {encoded['input_ids'][0].tolist()}\\n\")\n",
    "\n",
    "\n",
    "encoded = encoded.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Predictions: {predictions.tolist()[0]}\")\n",
    "\n",
    "del encoded, outputs, logits, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03248c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping:\n",
      "  % -> 0\n",
      "  b -> 1\n",
      "  fg -> 2\n",
      "  fh -> 3\n",
      "  h -> 4\n",
      "  qh -> 5\n",
      "  qo -> 6\n",
      "  qr -> 7\n",
      "  qrr -> 8\n",
      "  qw -> 9\n",
      "  qy -> 10\n",
      "  s -> 11\n",
      "\n",
      "Max length: 96\n"
     ]
    }
   ],
   "source": [
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(\"Label mapping:\")\n",
    "for label, idx in label2id.items():\n",
    "    print(f\"  {label} -> {idx}\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize text and encode labels\"\"\"\n",
    "    tokens = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,  # Will pad later in DataCollator\n",
    "        max_length=128,  # Actual max length is 96\n",
    "        return_tensors=None\n",
    "    )\n",
    "    tokens[\"labels\"] = [label2id[label] for label in examples[\"general_da\"]]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to all splits\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "# Verify no data loss\n",
    "if len(dataset[\"train\"]) != len(tokenized_datasets[\"train\"]):\n",
    "    print(f\"Not all samples were processed\\n\")\n",
    "    print(f\"Original train size: {len(dataset['train']):,}\")\n",
    "    print(f\"Processed train size: {len(tokenized_datasets['train']):,}\")\n",
    "if len(dataset[\"validation\"]) != len(tokenized_datasets[\"validation\"]):\n",
    "    print(f\"Not all samples were processed\\n\")\n",
    "    print(f\"Original validation size: {len(dataset['validation']):,}\")\n",
    "    print(f\"Processed validation size: {len(tokenized_datasets['validation']):,}\")\n",
    "if len(dataset[\"test\"]) != len(tokenized_datasets[\"test\"]):\n",
    "    print(f\"Not all samples were processed\\n\")\n",
    "    print(f\"Original test size: {len(dataset['test']):,}\")\n",
    "    print(f\"Processed test size: {len(tokenized_datasets['test']):,}\")\n",
    "\n",
    "# Check max length\n",
    "print(\"\\nMax length:\", max(max(len(s[\"input_ids\"]) for s in tokenized_datasets[split]) for split in [\"train\",\"validation\",\"test\"]))\n",
    "\n",
    "# Create data collator for batching\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b82e789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights (balanced method):\n",
      "  0 (%): 0.796 (count: 2171)\n",
      "  1 (b): 0.383 (count: 10606)\n",
      "  2 (fg): 0.820 (count: 2076)\n",
      "  3 (fh): 0.478 (count: 5617)\n",
      "  4 (h): 2.656 (count: 474)\n",
      "  5 (qh): 4.615 (count: 260)\n",
      "  6 (qo): 10.000 (count: 116)\n",
      "  7 (qr): 8.887 (count: 131)\n",
      "  8 (qrr): 4.899 (count: 244)\n",
      "  9 (qw): 1.293 (count: 1110)\n",
      "  10 (qy): 0.618 (count: 3310)\n",
      "  11 (s): 0.300 (count: 48952)\n"
     ]
    }
   ],
   "source": [
    "MAX_WEIGHT = 10\n",
    "MIN_WEIGHT = 0.3\n",
    "\n",
    "raw_train_labels = [sample[\"labels\"] for sample in tokenized_datasets[\"train\"]]\n",
    "total_train_examples = len(raw_train_labels)\n",
    "train_label_counts = dict(sorted(Counter(raw_train_labels).items(), key=lambda x: x[0]))\n",
    "\n",
    "# Compute weights\n",
    "class_weights = [1 / x for x in train_label_counts.values()]\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Scale weights to MAX_WEIGHT and MIN_WEIGHT\n",
    "curr_max_weight = class_weights_tensor.max()\n",
    "curr_min_weight = class_weights_tensor.min()\n",
    "class_weights_tensor = (class_weights_tensor - curr_min_weight) / (curr_max_weight - curr_min_weight) * (MAX_WEIGHT - MIN_WEIGHT) + MIN_WEIGHT\n",
    "\n",
    "print(f\"Class weights (balanced method):\")\n",
    "for class_id in range(NUM_LABELS):\n",
    "    print(f\"  {class_id} ({id2label[class_id]}): {class_weights_tensor[class_id].item() :.3f} (count: {train_label_counts[class_id]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99bce13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal loss with balanced cross entropy\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Custom Weighted Trainer\n",
    "class AdvancedTrainer(Trainer):\n",
    "    def __init__(self, loss_type=\"focal\", class_weights=None, focal_gamma=2.0, loss_reduction=\"mean\", *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_type = loss_type\n",
    "        self.class_weights = class_weights\n",
    "        \n",
    "        if loss_type == \"focal\":\n",
    "            self.loss_fn = FocalLoss(alpha=class_weights, gamma=focal_gamma, reduction=loss_reduction)\n",
    "        elif loss_type == \"weighted_ce\":\n",
    "            self.loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        else:\n",
    "            self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        loss = self.loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
    "    micro_f1 = f1_score(labels, predictions, average='micro')\n",
    "    weighted_f1 = f1_score(labels, predictions, average='weighted')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"weighted_f1\": weighted_f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "115113c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.00046061722708429296, 1: 9.428625306430322e-05, 2: 0.0004816955684007707, 3: 0.00017803097739006588, 4: 0.002109704641350211, 5: 0.0038461538461538464, 6: 0.008620689655172414, 7: 0.007633587786259542, 8: 0.004098360655737705, 9: 0.0009009009009009009, 10: 0.00030211480362537764, 11: 2.0428174538323256e-05}\n"
     ]
    }
   ],
   "source": [
    "# TODO the trainer isn't using stratified sampling\n",
    "# 4. Stratified Batch Sampling\n",
    "def create_stratified_sampler(dataset):\n",
    "    # Get labels from dataset\n",
    "    labels = [sample[\"labels\"] for sample in dataset]\n",
    "    \n",
    "    # Calculate sample weights for stratified sampling - handle non-contiguous labels\n",
    "    unique_labels_in_data = sorted(set(labels))\n",
    "    class_sample_count = {label: labels.count(label) for label in unique_labels_in_data}\n",
    "    \n",
    "    # Create weight mapping for each class\n",
    "    weight_mapping = {}\n",
    "    for label, count in class_sample_count.items():\n",
    "        weight_mapping[label] = 1.0 / count\n",
    "    \n",
    "    # Assign weight to each sample\n",
    "    samples_weight = torch.tensor([weight_mapping[label] for label in labels])\n",
    "    print(weight_mapping)\n",
    "    # Create sampler\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=samples_weight,\n",
    "        num_samples=len(samples_weight),\n",
    "        replacement=True\n",
    "    )\n",
    "    return sampler\n",
    "\n",
    "# Create stratified sampler\n",
    "train_sampler = create_stratified_sampler(tokenized_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff815852",
   "metadata": {},
   "source": [
    "### Parameters tuning\n",
    "**LoRA**\n",
    "- lora_dropout ‚Äì Regular dropout regularization, prevents overfitting. (0.0; 0.3)\n",
    "- lora_r ‚Äì The size of LoRA adapters, means how much new information the model can learn. High values cause better learning capacity, especially usefull for imbalanced data  (8; 64)\n",
    "- lora_alpha ‚Äì How strong the LoRA adapters influence the original model. Affects effective scaling (`lora_alpha/lora_r`) (scale between 0.5 and 8)\n",
    "\n",
    "**Traning arguments**\n",
    "\n",
    "NOTE: Step is calculated by `ceil(total_samples / batch_size)`\n",
    "\n",
    "Proper Warmup Guidelines:\n",
    "- Simple tasks: 5-10% of total steps\n",
    "- Complex tasks: 10-15% of total steps\n",
    "- Imbalanced/Difficult: 15-20% of total steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e2e9e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 14076\n",
      "Steps per epoch: 2346\n",
      "Steps per eval: 391\n",
      "Steps per logging: 195\n",
      "Steps per saving: 1173\n"
     ]
    }
   ],
   "source": [
    "LORA_DROPOUT = 0.15\n",
    "LORA_R = 64\n",
    "LORA_ALPHA = 128\n",
    "\n",
    "LOSS_TYPE = \"focal\"\n",
    "NO_EPOCHS = 6\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0001\n",
    "WARMUP_STEPS = 0.15\n",
    "WEIGHT_DECAY = 0.04\n",
    "\n",
    "FOCAL_GAMMA=2.0\n",
    "LOSS_REDUCTION=\"mean\"\n",
    "\n",
    "### LoRA setup ###\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    target_modules=[\"q_lin\", \"v_lin\", \"k_lin\", \"out_lin\"],\n",
    "    ### TUNABLE PARAMETERS ###\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    ")\n",
    "advanced_peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "### Training setup ###\n",
    "total_steps = NO_EPOCHS * np.ceil(len(tokenized_datasets[\"train\"]) / BATCH_SIZE)\n",
    "eval_steps = total_steps // NO_EPOCHS // 6 # 6 times per epoch\n",
    "save_steps = eval_steps * 3 # 3 times per epoch\n",
    "logging_steps = total_steps // NO_EPOCHS // 12 # 12 times per epoch\n",
    "advanced_training_args = TrainingArguments(\n",
    "    output_dir=\"./advanced_checkpoints\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",  # Optimize for macro F1, perfect for imbalanced data\n",
    "    greater_is_better=True,\n",
    "    report_to=None,\n",
    "    remove_unused_columns=False, # Required for custom trainer\n",
    "    dataloader_num_workers=0,  # Important for MPS compatibility\n",
    "    eval_steps=eval_steps,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    ### TUNABLE PARAMETERS ###\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NO_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=int(WARMUP_STEPS * total_steps), # Prevents overfitting, should be ~15% of total steps for imbalanced data\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "advanced_trainer = AdvancedTrainer(\n",
    "    loss_type=LOSS_TYPE,  # Focal loss for imbalanced data\n",
    "    class_weights=class_weights_tensor, # Weights for imbalanced data\n",
    "    model=advanced_peft_model,\n",
    "    args=advanced_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    ### TUNABLE PARAMETERS ###\n",
    "    focal_gamma=FOCAL_GAMMA,\n",
    "    loss_reduction=LOSS_REDUCTION,\n",
    ")\n",
    "\n",
    "print(f\"Total steps: {int(total_steps)}\")\n",
    "print(f\"Steps per epoch: {int(total_steps // NO_EPOCHS)}\")\n",
    "print(f\"Steps per eval: {int(eval_steps)}\")\n",
    "print(f\"Steps per logging: {int(logging_steps)}\")\n",
    "print(f\"Steps per saving: {int(save_steps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5d3c93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting advanced training with:\n",
      "{'max_weight': 10, 'min_weight': 0.3, 'lora_dropout': 0.15, 'lora_r': 64, 'lora_alpha': 128, 'loss_type': 'focal', 'no_epochs': 6, 'batch_size': 32, 'learning_rate': 0.0001, 'warmup_steps': 0.15, 'weight_decay': 0.04, 'focal_gamma': 2.0, 'loss_reduction': 'mean'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14076' max='14076' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14076/14076 17:01, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>0.447400</td>\n",
       "      <td>0.448056</td>\n",
       "      <td>0.771192</td>\n",
       "      <td>0.244184</td>\n",
       "      <td>0.771192</td>\n",
       "      <td>0.736809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>0.305400</td>\n",
       "      <td>0.302378</td>\n",
       "      <td>0.786345</td>\n",
       "      <td>0.423984</td>\n",
       "      <td>0.786345</td>\n",
       "      <td>0.784220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1173</td>\n",
       "      <td>0.198700</td>\n",
       "      <td>0.250338</td>\n",
       "      <td>0.762612</td>\n",
       "      <td>0.459591</td>\n",
       "      <td>0.762612</td>\n",
       "      <td>0.775827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1564</td>\n",
       "      <td>0.221000</td>\n",
       "      <td>0.205420</td>\n",
       "      <td>0.769671</td>\n",
       "      <td>0.469760</td>\n",
       "      <td>0.769671</td>\n",
       "      <td>0.779697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1955</td>\n",
       "      <td>0.197700</td>\n",
       "      <td>0.188483</td>\n",
       "      <td>0.757074</td>\n",
       "      <td>0.518845</td>\n",
       "      <td>0.757074</td>\n",
       "      <td>0.773048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2346</td>\n",
       "      <td>0.196800</td>\n",
       "      <td>0.195790</td>\n",
       "      <td>0.731333</td>\n",
       "      <td>0.501521</td>\n",
       "      <td>0.731333</td>\n",
       "      <td>0.753815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2737</td>\n",
       "      <td>0.184600</td>\n",
       "      <td>0.193951</td>\n",
       "      <td>0.760725</td>\n",
       "      <td>0.535065</td>\n",
       "      <td>0.760725</td>\n",
       "      <td>0.778131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3128</td>\n",
       "      <td>0.176500</td>\n",
       "      <td>0.201102</td>\n",
       "      <td>0.773626</td>\n",
       "      <td>0.549519</td>\n",
       "      <td>0.773626</td>\n",
       "      <td>0.788564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3519</td>\n",
       "      <td>0.196500</td>\n",
       "      <td>0.188723</td>\n",
       "      <td>0.779407</td>\n",
       "      <td>0.564617</td>\n",
       "      <td>0.779407</td>\n",
       "      <td>0.791606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3910</td>\n",
       "      <td>0.175700</td>\n",
       "      <td>0.189446</td>\n",
       "      <td>0.727013</td>\n",
       "      <td>0.533963</td>\n",
       "      <td>0.727013</td>\n",
       "      <td>0.753041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4301</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>0.188382</td>\n",
       "      <td>0.748129</td>\n",
       "      <td>0.556153</td>\n",
       "      <td>0.748129</td>\n",
       "      <td>0.775336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4692</td>\n",
       "      <td>0.151400</td>\n",
       "      <td>0.201651</td>\n",
       "      <td>0.759082</td>\n",
       "      <td>0.517545</td>\n",
       "      <td>0.759082</td>\n",
       "      <td>0.774376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5083</td>\n",
       "      <td>0.156100</td>\n",
       "      <td>0.197801</td>\n",
       "      <td>0.771010</td>\n",
       "      <td>0.572414</td>\n",
       "      <td>0.771010</td>\n",
       "      <td>0.787318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5474</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.232444</td>\n",
       "      <td>0.768515</td>\n",
       "      <td>0.550128</td>\n",
       "      <td>0.768515</td>\n",
       "      <td>0.781794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5865</td>\n",
       "      <td>0.140700</td>\n",
       "      <td>0.207464</td>\n",
       "      <td>0.761334</td>\n",
       "      <td>0.555148</td>\n",
       "      <td>0.761334</td>\n",
       "      <td>0.780237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6256</td>\n",
       "      <td>0.167600</td>\n",
       "      <td>0.194493</td>\n",
       "      <td>0.745755</td>\n",
       "      <td>0.544293</td>\n",
       "      <td>0.745755</td>\n",
       "      <td>0.771730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6647</td>\n",
       "      <td>0.147300</td>\n",
       "      <td>0.206628</td>\n",
       "      <td>0.749163</td>\n",
       "      <td>0.555009</td>\n",
       "      <td>0.749163</td>\n",
       "      <td>0.768513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7038</td>\n",
       "      <td>0.147800</td>\n",
       "      <td>0.191237</td>\n",
       "      <td>0.760786</td>\n",
       "      <td>0.561997</td>\n",
       "      <td>0.760786</td>\n",
       "      <td>0.778471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7429</td>\n",
       "      <td>0.121500</td>\n",
       "      <td>0.207256</td>\n",
       "      <td>0.785614</td>\n",
       "      <td>0.563105</td>\n",
       "      <td>0.785614</td>\n",
       "      <td>0.797995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7820</td>\n",
       "      <td>0.144300</td>\n",
       "      <td>0.195560</td>\n",
       "      <td>0.756405</td>\n",
       "      <td>0.548084</td>\n",
       "      <td>0.756405</td>\n",
       "      <td>0.778022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8211</td>\n",
       "      <td>0.105400</td>\n",
       "      <td>0.204344</td>\n",
       "      <td>0.777703</td>\n",
       "      <td>0.563400</td>\n",
       "      <td>0.777703</td>\n",
       "      <td>0.791882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8602</td>\n",
       "      <td>0.144800</td>\n",
       "      <td>0.197806</td>\n",
       "      <td>0.744234</td>\n",
       "      <td>0.550726</td>\n",
       "      <td>0.744234</td>\n",
       "      <td>0.767400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8993</td>\n",
       "      <td>0.137200</td>\n",
       "      <td>0.198939</td>\n",
       "      <td>0.770036</td>\n",
       "      <td>0.567188</td>\n",
       "      <td>0.770036</td>\n",
       "      <td>0.786330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9384</td>\n",
       "      <td>0.144200</td>\n",
       "      <td>0.195689</td>\n",
       "      <td>0.771131</td>\n",
       "      <td>0.556824</td>\n",
       "      <td>0.771131</td>\n",
       "      <td>0.787539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9775</td>\n",
       "      <td>0.109400</td>\n",
       "      <td>0.193926</td>\n",
       "      <td>0.742287</td>\n",
       "      <td>0.542155</td>\n",
       "      <td>0.742287</td>\n",
       "      <td>0.769749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10166</td>\n",
       "      <td>0.123600</td>\n",
       "      <td>0.205555</td>\n",
       "      <td>0.774722</td>\n",
       "      <td>0.562450</td>\n",
       "      <td>0.774722</td>\n",
       "      <td>0.790472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10557</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>0.200692</td>\n",
       "      <td>0.763281</td>\n",
       "      <td>0.559977</td>\n",
       "      <td>0.763281</td>\n",
       "      <td>0.782942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10948</td>\n",
       "      <td>0.104700</td>\n",
       "      <td>0.216467</td>\n",
       "      <td>0.746486</td>\n",
       "      <td>0.560051</td>\n",
       "      <td>0.746486</td>\n",
       "      <td>0.775229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11339</td>\n",
       "      <td>0.092100</td>\n",
       "      <td>0.200396</td>\n",
       "      <td>0.764863</td>\n",
       "      <td>0.561167</td>\n",
       "      <td>0.764863</td>\n",
       "      <td>0.783401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11730</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>0.202711</td>\n",
       "      <td>0.775939</td>\n",
       "      <td>0.571370</td>\n",
       "      <td>0.775939</td>\n",
       "      <td>0.790510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12121</td>\n",
       "      <td>0.110100</td>\n",
       "      <td>0.204928</td>\n",
       "      <td>0.764742</td>\n",
       "      <td>0.559457</td>\n",
       "      <td>0.764742</td>\n",
       "      <td>0.782499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12512</td>\n",
       "      <td>0.095800</td>\n",
       "      <td>0.201034</td>\n",
       "      <td>0.757378</td>\n",
       "      <td>0.548535</td>\n",
       "      <td>0.757378</td>\n",
       "      <td>0.777016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12903</td>\n",
       "      <td>0.092500</td>\n",
       "      <td>0.205579</td>\n",
       "      <td>0.769001</td>\n",
       "      <td>0.562829</td>\n",
       "      <td>0.769001</td>\n",
       "      <td>0.787252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13294</td>\n",
       "      <td>0.117600</td>\n",
       "      <td>0.211020</td>\n",
       "      <td>0.769184</td>\n",
       "      <td>0.560095</td>\n",
       "      <td>0.769184</td>\n",
       "      <td>0.786288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13685</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>0.207898</td>\n",
       "      <td>0.766689</td>\n",
       "      <td>0.562889</td>\n",
       "      <td>0.766689</td>\n",
       "      <td>0.784827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14076</td>\n",
       "      <td>0.110400</td>\n",
       "      <td>0.207358</td>\n",
       "      <td>0.766567</td>\n",
       "      <td>0.563861</td>\n",
       "      <td>0.766567</td>\n",
       "      <td>0.784740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advanced training completed!\n",
      "Final train loss: 0.1602\n"
     ]
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    \"max_weight\": MAX_WEIGHT,\n",
    "    \"min_weight\": MIN_WEIGHT,\n",
    "\n",
    "    \"lora_dropout\": LORA_DROPOUT,\n",
    "    \"lora_r\": LORA_R,\n",
    "    \"lora_alpha\": LORA_ALPHA, \n",
    "\n",
    "    \"loss_type\": LOSS_TYPE,\n",
    "    \"no_epochs\": NO_EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"warmup_steps\": WARMUP_STEPS,\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "\n",
    "    \"focal_gamma\": FOCAL_GAMMA,\n",
    "    \"loss_reduction\": LOSS_REDUCTION,\n",
    "}\n",
    "\n",
    "print(f\"\\nStarting advanced training with:\")\n",
    "print(hyperparams)\n",
    "\n",
    "# Run advanced training\n",
    "advanced_result = advanced_trainer.train()\n",
    "\n",
    "print(f\"\\nAdvanced training completed!\")\n",
    "print(f\"Final train loss: {advanced_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b212de9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advanced evaluation results:\n",
      "  Accuracy: 0.7794\n",
      "  Macro F1: 0.5646\n",
      "  Micro F1: 0.7794\n",
      "  Weighted F1: 0.7916\n",
      "\n",
      "Prediction diversity analysis:\n",
      "  Unique classes predicted: 12/12\n",
      "  Prediction distribution:\n",
      "    0 (%): 918 predictions (5.6%)\n",
      "    1 (b): 3114 predictions (18.9%)\n",
      "    2 (fg): 590 predictions (3.6%)\n",
      "    3 (fh): 875 predictions (5.3%)\n",
      "    4 (h): 509 predictions (3.1%)\n",
      "    5 (qh): 28 predictions (0.2%)\n",
      "    6 (qo): 24 predictions (0.1%)\n",
      "    7 (qr): 62 predictions (0.4%)\n",
      "    8 (qrr): 106 predictions (0.6%)\n",
      "    9 (qw): 341 predictions (2.1%)\n",
      "    10 (qy): 805 predictions (4.9%)\n",
      "    11 (s): 9061 predictions (55.1%)\n",
      "\n",
      "Comparison with baseline:\n",
      "  Baseline accuracy: 65.0% (predicting only 1 class)\n",
      "  Advanced accuracy: 77.9%\n",
      "  Baseline macro F1: ~0.08 (random)\n",
      "  Advanced macro F1: 0.565\n",
      "  Prediction diversity: 12/12 classes vs 1/12 baseline\n",
      "‚úÖ Advanced training SUCCESS! Macro F1 > 0.3\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive evaluation\n",
    "advanced_eval = advanced_trainer.evaluate()\n",
    "print(f\"\\nAdvanced evaluation results:\")\n",
    "print(f\"  Accuracy: {advanced_eval['eval_accuracy']:.4f}\")\n",
    "print(f\"  Macro F1: {advanced_eval['eval_macro_f1']:.4f}\")\n",
    "print(f\"  Micro F1: {advanced_eval['eval_micro_f1']:.4f}\")\n",
    "print(f\"  Weighted F1: {advanced_eval['eval_weighted_f1']:.4f}\")\n",
    "\n",
    "# Check prediction diversity\n",
    "print(f\"\\nPrediction diversity analysis:\")\n",
    "advanced_predictions = advanced_trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predicted_classes = np.argmax(advanced_predictions.predictions, axis=1)\n",
    "unique_predictions = len(set(predicted_classes))\n",
    "predicted_counts = pd.Series(predicted_classes).value_counts().sort_index()\n",
    "\n",
    "print(f\"  Unique classes predicted: {unique_predictions}/12\")\n",
    "print(f\"  Prediction distribution:\")\n",
    "for label_id, count in predicted_counts.items():\n",
    "    label_name = id2label[label_id]\n",
    "    percentage = (count / len(predicted_classes)) * 100\n",
    "    print(f\"    {label_id} ({label_name}): {count} predictions ({percentage:.1f}%)\")\n",
    "\n",
    "# Compare with baseline (Step 6 results)\n",
    "print(f\"\\nComparison with baseline:\")\n",
    "print(f\"  Baseline accuracy: 65.0% (predicting only 1 class)\")\n",
    "print(f\"  Advanced accuracy: {advanced_eval['eval_accuracy']*100:.1f}%\")\n",
    "print(f\"  Baseline macro F1: ~0.08 (random)\")\n",
    "print(f\"  Advanced macro F1: {advanced_eval['eval_macro_f1']:.3f}\")\n",
    "print(f\"  Prediction diversity: {unique_predictions}/12 classes vs 1/12 baseline\")\n",
    "\n",
    "if advanced_eval['eval_macro_f1'] > 0.3:\n",
    "    print(\"‚úÖ Advanced training SUCCESS! Macro F1 > 0.3\")\n",
    "elif unique_predictions > 5:\n",
    "    print(\"‚úÖ Good prediction diversity! Learning multiple classes\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  May need further tuning - try weighted_ce loss or adjust gamma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2491ff90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PER-CLASS CONFUSION ANALYSIS (12 TABLES)\n",
      "================================================================================\n",
      "\n",
      "üìä CLASS 0: '%' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP= 341 | FP= 577\n",
      "                FN=  99 | TN=15416\n",
      "  Metrics:      Prec=0.371 | Rec=0.775 | F1=0.502\n",
      "  Distribution: True=  2.7% | Pred=  5.6% | Diff= +2.9%\n",
      "\n",
      "üìä CLASS 1: 'b' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP=2076 | FP=1038\n",
      "                FN= 266 | TN=13053\n",
      "  Metrics:      Prec=0.667 | Rec=0.886 | F1=0.761\n",
      "  Distribution: True= 14.3% | Pred= 18.9% | Diff= +4.7%\n",
      "\n",
      "üìä CLASS 2: 'fg' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP= 142 | FP= 448\n",
      "                FN= 385 | TN=15458\n",
      "  Metrics:      Prec=0.241 | Rec=0.269 | F1=0.254\n",
      "  Distribution: True=  3.2% | Pred=  3.6% | Diff= +0.4%\n",
      "\n",
      "üìä CLASS 3: 'fh' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP= 601 | FP= 274\n",
      "                FN= 624 | TN=14934\n",
      "  Metrics:      Prec=0.687 | Rec=0.491 | F1=0.572\n",
      "  Distribution: True=  7.5% | Pred=  5.3% | Diff= -2.1%\n",
      "\n",
      "üìä CLASS 4: 'h' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP=  92 | FP= 417\n",
      "                FN=  92 | TN=15832\n",
      "  Metrics:      Prec=0.181 | Rec=0.500 | F1=0.266\n",
      "  Distribution: True=  1.1% | Pred=  3.1% | Diff= +2.0%\n",
      "\n",
      "üìä CLASS 5: 'qh' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP=   7 | FP=  21\n",
      "                FN=  29 | TN=16376\n",
      "  Metrics:      Prec=0.250 | Rec=0.194 | F1=0.219\n",
      "  Distribution: True=  0.2% | Pred=  0.2% | Diff= -0.0%\n",
      "\n",
      "üìä CLASS 6: 'qo' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP=   9 | FP=  15\n",
      "                FN=  16 | TN=16393\n",
      "  Metrics:      Prec=0.375 | Rec=0.360 | F1=0.367\n",
      "  Distribution: True=  0.2% | Pred=  0.1% | Diff= -0.0%\n",
      "\n",
      "üìä CLASS 7: 'qr' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP=  27 | FP=  35\n",
      "                FN=  12 | TN=16359\n",
      "  Metrics:      Prec=0.435 | Rec=0.692 | F1=0.535\n",
      "  Distribution: True=  0.2% | Pred=  0.4% | Diff= +0.1%\n",
      "\n",
      "üìä CLASS 8: 'qrr' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP=  68 | FP=  38\n",
      "                FN=   5 | TN=16322\n",
      "  Metrics:      Prec=0.642 | Rec=0.932 | F1=0.760\n",
      "  Distribution: True=  0.4% | Pred=  0.6% | Diff= +0.2%\n",
      "\n",
      "üìä CLASS 9: 'qw' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP= 243 | FP=  98\n",
      "                FN=  44 | TN=16048\n",
      "  Metrics:      Prec=0.713 | Rec=0.847 | F1=0.774\n",
      "  Distribution: True=  1.7% | Pred=  2.1% | Diff= +0.3%\n",
      "\n",
      "üìä CLASS 10: 'qy' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP= 722 | FP=  83\n",
      "                FN=  84 | TN=15544\n",
      "  Metrics:      Prec=0.897 | Rec=0.896 | F1=0.896\n",
      "  Distribution: True=  4.9% | Pred=  4.9% | Diff= -0.0%\n",
      "\n",
      "üìä CLASS 11: 's' Analysis\n",
      "--------------------------------------------------\n",
      "  Confusion:    TP=8480 | FP= 581\n",
      "                FN=1969 | TN=5403\n",
      "  Metrics:      Prec=0.936 | Rec=0.812 | F1=0.869\n",
      "  Distribution: True= 63.6% | Pred= 55.1% | Diff= -8.4%\n",
      "\n",
      "üéØ OVERALL: Acc=0.779 | MacroF1=0.565 | WeightedF1=0.792\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def setup_experiment_logging(experiment_name, hyperparams):\n",
    "    \"\"\"Setup logging for parallel experiments\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_dir = f\"results/{experiment_name}_{timestamp}\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Save hyperparameters\n",
    "    with open(f\"{results_dir}/hyperparams.json\", 'w') as f:\n",
    "        json.dump(hyperparams, f, indent=2)\n",
    "    \n",
    "    return results_dir\n",
    "\n",
    "def comprehensive_evaluation(trainer, tokenized_datasets, id2label, results_dir):\n",
    "    \"\"\"Complete evaluation with confusion matrix and detailed 12-class analysis\"\"\"\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "    y_true = predictions.label_ids\n",
    "    y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "    \n",
    "    # Overall metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'macro_f1': f1_score(y_true, y_pred, average='macro'),\n",
    "        'weighted_f1': f1_score(y_true, y_pred, average='weighted'),\n",
    "    }\n",
    "    \n",
    "    # Per-class metrics (ESSENTIAL for imbalanced data)\n",
    "    class_report = classification_report(y_true, y_pred, \n",
    "                                       target_names=[id2label[i] for i in range(len(id2label))],\n",
    "                                       output_dict=True)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # ===== DETAILED 12-CLASS ANALYSIS =====\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PER-CLASS CONFUSION ANALYSIS (12 TABLES)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    detailed_analysis = {}\n",
    "    for class_id in range(len(id2label)):\n",
    "        class_name = id2label[class_id]\n",
    "        \n",
    "        # True/False positives/negatives for this class\n",
    "        tp = cm[class_id, class_id]\n",
    "        fp = cm[:, class_id].sum() - tp\n",
    "        fn = cm[class_id, :].sum() - tp\n",
    "        tn = cm.sum() - tp - fp - fn\n",
    "        \n",
    "        # Metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # Distribution\n",
    "        true_count = (y_true == class_id).sum()\n",
    "        pred_count = (y_pred == class_id).sum()\n",
    "        true_pct = (true_count / len(y_true)) * 100\n",
    "        pred_pct = (pred_count / len(y_pred)) * 100\n",
    "        \n",
    "        detailed_analysis[class_name] = {\n",
    "            'tp': int(tp), 'fp': int(fp), 'fn': int(fn), 'tn': int(tn),\n",
    "            'precision': precision, 'recall': recall, 'f1': f1,\n",
    "            'true_pct': true_pct, 'pred_pct': pred_pct, 'diff_pct': pred_pct - true_pct\n",
    "        }\n",
    "        \n",
    "        # Print individual class table\n",
    "        print(f\"\\nüìä CLASS {class_id}: '{class_name}' Analysis\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"  Confusion:    TP={tp:4d} | FP={fp:4d}\")\n",
    "        print(f\"                FN={fn:4d} | TN={tn:4d}\")\n",
    "        print(f\"  Metrics:      Prec={precision:.3f} | Rec={recall:.3f} | F1={f1:.3f}\")\n",
    "        print(f\"  Distribution: True={true_pct:5.1f}% | Pred={pred_pct:5.1f}% | Diff={pred_pct-true_pct:+5.1f}%\")\n",
    "    \n",
    "    # Class distribution analysis\n",
    "    true_dist = pd.Series(y_true).value_counts().sort_index()\n",
    "    pred_dist = pd.Series(y_pred).value_counts().sort_index()\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'overall_metrics': metrics,\n",
    "        'per_class_metrics': class_report,\n",
    "        'detailed_class_analysis': detailed_analysis,  # Added detailed analysis\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'true_distribution': true_dist.to_dict(),\n",
    "        'pred_distribution': pred_dist.to_dict()\n",
    "    }\n",
    "    \n",
    "    with open(f\"{results_dir}/evaluation.json\", 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüéØ OVERALL: Acc={metrics['accuracy']:.3f} | MacroF1={metrics['macro_f1']:.3f} | WeightedF1={metrics['weighted_f1']:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "results_dir = setup_experiment_logging(\"no_1\", hyperparams)\n",
    "results = comprehensive_evaluation(advanced_trainer, tokenized_datasets, id2label, results_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
