{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6282955",
   "metadata": {},
   "source": [
    "# MRDA Model Loading and Inference\n",
    "\n",
    "This notebook loads the fine-tuned DistilBERT model with LoRA adapters for 12-class dialogue act classification.\n",
    "\n",
    "**Model Details:**\n",
    "- Base Model: `distilbert-base-uncased`\n",
    "- Fine-tuned with LoRA adapters \n",
    "- Task: 12-class MRDA dialogue act classification\n",
    "- Classes: `['%', 'b', 'fg', 'fh', 'h', 'qh', 'qo', 'qr', 'qrr', 'qw', 'qy', 's']`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcf6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3d832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_device():\n",
    "    \"\"\"Detect best available device with fallback strategy\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps\" \n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    return device\n",
    "\n",
    "device = detect_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bb6d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"../advanced_checkpoints/checkpoint-11730\"\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "# NUM_LABELS = 12\n",
    "NUM_LABELS = 5\n",
    "\n",
    "# Label mappings from your training\n",
    "# unique_labels = ['%', 'b', 'fg', 'fh', 'h', 'qh', 'qo', 'qr', 'qrr', 'qw', 'qy', 's']\n",
    "unique_labels = ['S', 'Q', 'D', 'F', 'B']\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_PATH)\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"single_label_classification\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, CHECKPOINT_PATH)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf86323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTENT_LABELS = {'s', 'qy', 'qw', 'qh', 'qrr', 'qr', 'qo'}\n",
    "# NON_CONTENT_LABELS = {'b', 'fh', 'fg', '%', 'h'}\n",
    "# LABEL_DESCRIPTIONS = {\n",
    "#     '%': 'Interrupted/Abandoned utterance',\n",
    "#     'b': 'Continuer (backchannel)',\n",
    "#     'fg': 'Floor Grabber (taking the floor)',\n",
    "#     'fh': 'Floor Holder (keeping the floor)', \n",
    "#     'h': 'Hold Before Answer (hesitation)',\n",
    "#     'qh': 'Rhetorical Question',\n",
    "#     'qo': 'Open-ended Question',\n",
    "#     'qr': 'Or Question',\n",
    "#     'qrr': 'Or-Clause (part of question)',\n",
    "#     'qw': 'Wh-Question (what, where, when, etc.)',\n",
    "#     'qy': 'Yes-No Question',\n",
    "#     's': 'Statement'\n",
    "# }\n",
    "CONTENT_LABELS = {'S', 'Q'}\n",
    "NON_CONTENT_LABELS = {'B', 'D', 'F'}\n",
    "LABEL_DESCRIPTIONS = {\n",
    "    'S': 'Statement',\n",
    "    'Q': 'Question',\n",
    "    'B': 'Backchannel',\n",
    "    'D': 'Disruptions',\n",
    "    'F': 'Floor Grabber'\n",
    "}\n",
    "\n",
    "def map_to_binary(general_da_label):\n",
    "    \"\"\"Map general DA label to binary content/non-content\"\"\"\n",
    "    if general_da_label in CONTENT_LABELS:\n",
    "        return 1, \"content\"\n",
    "    elif general_da_label in NON_CONTENT_LABELS:\n",
    "        return 0, \"non-content\"\n",
    "    else:\n",
    "        return -1, \"unknown\"\n",
    "\n",
    "def predict_single(text, return_probabilities=False):\n",
    "    \"\"\"\n",
    "    Predict dialogue act for a single text\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        return_probabilities (bool): Whether to return class probabilities\n",
    "    \n",
    "    Returns:\n",
    "        dict: Prediction results\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    ).to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        predicted_class_id = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    # Get predicted label\n",
    "    predicted_label = id2label[predicted_class_id]\n",
    "    confidence = probabilities[0][predicted_class_id].item()\n",
    "    \n",
    "    # Binary classification\n",
    "    binary_label, binary_text = map_to_binary(predicted_label)\n",
    "    \n",
    "    result = {\n",
    "        'text': text,\n",
    "        'predicted_class_id': predicted_class_id,\n",
    "        'predicted_label': predicted_label,\n",
    "        'confidence': confidence,\n",
    "        'binary_label': binary_label,\n",
    "        'binary_text': binary_text\n",
    "    }\n",
    "    \n",
    "    if return_probabilities:\n",
    "        all_probs = {id2label[i]: prob.item() for i, prob in enumerate(probabilities[0])}\n",
    "        result['all_probabilities'] = all_probs\n",
    "    \n",
    "    return result\n",
    "\n",
    "def predict_batch_real(texts, return_probabilities=False):\n",
    "    \"\"\"\n",
    "    TRUE batch prediction - processes all texts in single forward pass\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of input texts\n",
    "        return_probabilities (bool): Whether to return class probabilities\n",
    "    \n",
    "    Returns:\n",
    "        list: List of prediction results\n",
    "    \"\"\"\n",
    "    if not texts:\n",
    "        return []\n",
    "    \n",
    "    # Tokenize all texts together - this is the key for real batching\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    ).to(device)\n",
    "    \n",
    "    # Single forward pass for all texts\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        predicted_class_ids = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Process results for each text\n",
    "    results = []\n",
    "    for i, text in enumerate(texts):\n",
    "        predicted_class_id = predicted_class_ids[i].item()\n",
    "        predicted_label = id2label[predicted_class_id]\n",
    "        confidence = probabilities[i][predicted_class_id].item()\n",
    "        \n",
    "        # Binary classification\n",
    "        binary_label, binary_text = map_to_binary(predicted_label)\n",
    "        \n",
    "        result = {\n",
    "            'text': text,\n",
    "            'predicted_class_id': predicted_class_id,\n",
    "            'predicted_label': predicted_label,\n",
    "            'confidence': confidence,\n",
    "            'binary_label': binary_label,\n",
    "            'binary_text': binary_text\n",
    "        }\n",
    "        \n",
    "        if return_probabilities:\n",
    "            all_probs = {id2label[j]: prob.item() for j, prob in enumerate(probabilities[i])}\n",
    "            result['all_probabilities'] = all_probs\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def predict_batch(texts, return_probabilities=False):\n",
    "    \"\"\"\n",
    "    FAKE batch prediction - just loops through predict_single (kept for compatibility)\n",
    "    Use predict_batch_real() for true batching\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        result = predict_single(text, return_probabilities)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "def display_prediction(result, true_label=None):\n",
    "    \"\"\"\n",
    "    Display prediction results in a nice format\n",
    "    \n",
    "    Args:\n",
    "        result (dict): Prediction result from predict_single or predict_batch_real\n",
    "        true_label (str, optional): Ground truth label for comparison\n",
    "    \"\"\"\n",
    "    print(f\"üìù Text: '{result['text']}'\")\n",
    "    print(f\"üè∑Ô∏è Predicted: {result['predicted_label']} (ID: {result['predicted_class_id']})\")\n",
    "    # Show ground truth if provided\n",
    "    if true_label is not None:\n",
    "        is_correct = result['predicted_label'] == true_label\n",
    "        status_icon = \"‚úÖ\" if is_correct else \"‚ùå\"\n",
    "        print(f\"üéØ True Label: {true_label} {status_icon}\")\n",
    "        if not is_correct:\n",
    "            print(f\"   True Description: {LABEL_DESCRIPTIONS.get(true_label, 'Unknown')}\")\n",
    "            true_binary = map_to_binary(true_label)[1]\n",
    "            pred_binary = result['binary_text']\n",
    "            binary_correct = \"‚úÖ\" if true_binary == pred_binary else \"‚ùå\"\n",
    "            print(f\"   Binary: {true_binary} vs {pred_binary} {binary_correct}\")\n",
    "    \n",
    "    print(f\"üìä Confidence: {result['confidence']:.4f}\")\n",
    "    print(f\"üîÑ Binary Classification: {result['binary_text']}\")\n",
    "    print(f\"üí° Description: {LABEL_DESCRIPTIONS[result['predicted_label']]}\")\n",
    "\n",
    "    if 'all_probabilities' in result:\n",
    "        print(f\"üìà Top 5 Probabilities:\")\n",
    "        sorted_probs = sorted(result['all_probabilities'].items(), key=lambda x: x[1], reverse=True)\n",
    "        for label, prob in sorted_probs[:5]:\n",
    "            print(f\"   {label}: {prob:.4f}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "def display_batch_predictions(results, true_labels=None):\n",
    "    \"\"\"\n",
    "    Display batch prediction results\n",
    "    \n",
    "    Args:\n",
    "        results (list): List of prediction results\n",
    "        true_labels (list, optional): List of ground truth labels for comparison\n",
    "    \"\"\"\n",
    "    correct_count = 0\n",
    "    total_count = len(results)\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        true_label = true_labels[i] if true_labels and i < len(true_labels) else None\n",
    "        display_prediction(result, true_label)\n",
    "        \n",
    "        if true_label and result['predicted_label'] == true_label:\n",
    "            correct_count += 1\n",
    "    \n",
    "    # Show summary if true labels provided\n",
    "    if true_labels:\n",
    "        accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "        print(f\"üìä BATCH SUMMARY: {correct_count}/{total_count} correct ({accuracy:.1%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40473ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = predict_single(\"I was fine-tuning BERT\", return_probabilities=True)\n",
    "display_prediction(x, true_label=\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f692f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [\n",
    "    \"okay\",  # Should be 'fg' (Floor Grabber) - non-content\n",
    "    \"i think that's a good idea\",  # Should be 's' (Statement) - content\n",
    "    \"uh-huh\",  # Should be 'b' (Continuer) - non-content  \n",
    "    \"what do you think about this?\",  # Should be 'qy' (Yes-No Question) - content\n",
    "    \"how are you doing?\",  # Should be 'qw' (Wh-Question) - content\n",
    "    \"um\",  # Should be 'fh' (Floor Holder) - non-content\n",
    "]\n",
    "labels = [\n",
    "    \"fg\",\n",
    "    \"s\",\n",
    "    \"b\",\n",
    "    \"qy\",\n",
    "    \"qw\",\n",
    "    \"fh\",\n",
    "]\n",
    "\n",
    "\n",
    "predictions = predict_batch_real(batch, return_probabilities=True)\n",
    "display_batch_predictions(predictions,  true_labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba75f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with example utterances\n",
    "test_utterances = [\n",
    "    \"okay\",  # Should be 'fg' (Floor Grabber) - non-content\n",
    "    \"i think that's a good idea\",  # Should be 's' (Statement) - content\n",
    "    \"uh-huh\",  # Should be 'b' (Continuer) - non-content  \n",
    "    \"what do you think about this?\",  # Should be 'qy' (Yes-No Question) - content\n",
    "    \"how are you doing?\",  # Should be 'qw' (Wh-Question) - content\n",
    "    \"um\",  # Should be 'fh' (Floor Holder) - non-content\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing model with example utterances:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for utterance in test_utterances:\n",
    "    result = predict_single(utterance, return_probabilities=True)\n",
    "    display_prediction(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c0b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate TRUE batch processing vs FAKE batch processing\n",
    "import time\n",
    "\n",
    "# Test data\n",
    "test_texts = [\n",
    "    \"okay let's start\",\n",
    "    \"i think this is important\", \n",
    "    \"uh-huh\",\n",
    "    \"what should we do next?\",\n",
    "    \"how about this approach?\",\n",
    "    \"um let me think\",\n",
    "    \"that sounds good\",\n",
    "    \"yes exactly\",\n",
    "    \"are you sure about that?\",\n",
    "    \"maybe we should consider\"\n",
    "] * 5  # 50 texts total\n",
    "test_labels = [\"fg\", \"s\", \"b\", \"qw\", \"qw\", \"fh\", \"s\", \"b\", \"qy\", \"s\"] * 5\n",
    "\n",
    "print(f\"üß™ Comparing batch processing methods with {len(test_texts)} texts:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# FAKE batch processing (loops through predict_single)\n",
    "start_time = time.time()\n",
    "fake_results = predict_batch(test_texts, return_probabilities=False)\n",
    "fake_time = time.time() - start_time\n",
    "\n",
    "# TRUE batch processing (single forward pass)  \n",
    "start_time = time.time()\n",
    "real_results = predict_batch_real(test_texts, return_probabilities=False)\n",
    "real_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚ö° FAKE Batch (loops): {fake_time:.3f}s\")\n",
    "print(f\"üöÄ REAL Batch (vectorized): {real_time:.3f}s\")\n",
    "print(f\"üí® Speedup: {fake_time/real_time:.1f}x faster!\")\n",
    "\n",
    "# Verify results are identical\n",
    "predictions_match = all(\n",
    "    fake['predicted_label'] == real['predicted_label'] \n",
    "    for fake, real in zip(fake_results, real_results)\n",
    ")\n",
    "print(f\"‚úÖ Results identical: {predictions_match}\")\n",
    "\n",
    "# Show accuracy for both methods\n",
    "fake_correct = sum(1 for i, result in enumerate(fake_results) \n",
    "                   if result['predicted_label'] == test_labels[i])\n",
    "real_correct = sum(1 for i, result in enumerate(real_results) \n",
    "                   if result['predicted_label'] == test_labels[i])\n",
    "\n",
    "print(f\"üìä Accuracy: {fake_correct}/{len(test_texts)} = {fake_correct/len(test_texts):.1%}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ Always use predict_batch_real() for better performance!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e97203",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../transcript_en.txt\", \"r\") as file:\n",
    "    transcript = file.read()\n",
    "\n",
    "transcript = transcript.split(\"\\n\")\n",
    "transcript = [line.split(\";\") for line in transcript]\n",
    "transcript_texts = [line[1] for line in transcript]\n",
    "\n",
    "start_time = time.time()\n",
    "real_results = predict_batch_real(transcript_texts, return_probabilities=False)\n",
    "real_time = time.time() - start_time\n",
    "\n",
    "print(f\"Time: {real_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63dcd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result, text in zip(real_results, transcript_texts):\n",
    "    print(text[0])\n",
    "    print(result)\n",
    "    print(text[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
