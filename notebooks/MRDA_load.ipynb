{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6282955",
   "metadata": {},
   "source": [
    "# MRDA Model Loading and Inference\n",
    "\n",
    "This notebook loads the fine-tuned DistilBERT model with LoRA adapters for 12-class dialogue act classification.\n",
    "\n",
    "**Model Details:**\n",
    "- Base Model: `distilbert-base-uncased`\n",
    "- Fine-tuned with LoRA adapters \n",
    "- Task: 12-class MRDA dialogue act classification\n",
    "- Classes: `['%', 'b', 'fg', 'fh', 'h', 'qh', 'qo', 'qr', 'qrr', 'qw', 'qy', 's']`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dcf6ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filip/Documents/github/IE-pipeline/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d3d832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_device():\n",
    "    \"\"\"Detect best available device with fallback strategy\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps\" \n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    return device\n",
    "\n",
    "device = detect_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64bb6d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded successfully!\n",
      "   ğŸ“¦ Base model: distilbert-base-uncased\n",
      "   ğŸ¯ Task: 12-class dialogue act classification\n",
      "   ğŸ“Š Parameters: 69,921,816\n",
      "   ğŸ”§ Trainable parameters: 599,820\n",
      "   ğŸ’¾ Device: cuda\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH = \"../advanced_checkpoints/checkpoint-11730\"\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "NUM_LABELS = 12\n",
    "\n",
    "# Label mappings from your training\n",
    "unique_labels = ['%', 'b', 'fg', 'fh', 'h', 'qh', 'qo', 'qr', 'qrr', 'qw', 'qy', 's']\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_PATH)\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"single_label_classification\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, CHECKPOINT_PATH)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"âœ… Model loaded successfully!\")\n",
    "print(f\"   ğŸ“¦ Base model: {MODEL_NAME}\")\n",
    "print(f\"   ğŸ¯ Task: 12-class dialogue act classification\")\n",
    "print(f\"   ğŸ“Š Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   ğŸ”§ Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"   ğŸ’¾ Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cf86323",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_LABELS = {'s', 'qy', 'qw', 'qh', 'qrr', 'qr', 'qo'}\n",
    "NON_CONTENT_LABELS = {'b', 'fh', 'fg', '%', 'h'}\n",
    "LABEL_DESCRIPTIONS = {\n",
    "    '%': 'Interrupted/Abandoned utterance',\n",
    "    'b': 'Continuer (backchannel)',\n",
    "    'fg': 'Floor Grabber (taking the floor)',\n",
    "    'fh': 'Floor Holder (keeping the floor)', \n",
    "    'h': 'Hold Before Answer (hesitation)',\n",
    "    'qh': 'Rhetorical Question',\n",
    "    'qo': 'Open-ended Question',\n",
    "    'qr': 'Or Question',\n",
    "    'qrr': 'Or-Clause (part of question)',\n",
    "    'qw': 'Wh-Question (what, where, when, etc.)',\n",
    "    'qy': 'Yes-No Question',\n",
    "    's': 'Statement'\n",
    "}\n",
    "\n",
    "\n",
    "def map_to_binary(general_da_label):\n",
    "    \"\"\"Map general DA label to binary content/non-content\"\"\"\n",
    "    if general_da_label in CONTENT_LABELS:\n",
    "        return 1, \"content\"\n",
    "    elif general_da_label in NON_CONTENT_LABELS:\n",
    "        return 0, \"non-content\"\n",
    "    else:\n",
    "        return -1, \"unknown\"\n",
    "\n",
    "def predict_single(text, return_probabilities=False):\n",
    "    \"\"\"\n",
    "    Predict dialogue act for a single text\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        return_probabilities (bool): Whether to return class probabilities\n",
    "    \n",
    "    Returns:\n",
    "        dict: Prediction results\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    ).to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        predicted_class_id = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    # Get predicted label\n",
    "    predicted_label = id2label[predicted_class_id]\n",
    "    confidence = probabilities[0][predicted_class_id].item()\n",
    "    \n",
    "    # Binary classification\n",
    "    binary_label, binary_text = map_to_binary(predicted_label)\n",
    "    \n",
    "    result = {\n",
    "        'text': text,\n",
    "        'predicted_class_id': predicted_class_id,\n",
    "        'predicted_label': predicted_label,\n",
    "        'confidence': confidence,\n",
    "        'binary_label': binary_label,\n",
    "        'binary_text': binary_text\n",
    "    }\n",
    "    \n",
    "    if return_probabilities:\n",
    "        all_probs = {id2label[i]: prob.item() for i, prob in enumerate(probabilities[0])}\n",
    "        result['all_probabilities'] = all_probs\n",
    "    \n",
    "    return result\n",
    "\n",
    "def predict_batch_real(texts, return_probabilities=False):\n",
    "    \"\"\"\n",
    "    TRUE batch prediction - processes all texts in single forward pass\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of input texts\n",
    "        return_probabilities (bool): Whether to return class probabilities\n",
    "    \n",
    "    Returns:\n",
    "        list: List of prediction results\n",
    "    \"\"\"\n",
    "    if not texts:\n",
    "        return []\n",
    "    \n",
    "    # Tokenize all texts together - this is the key for real batching\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    ).to(device)\n",
    "    \n",
    "    # Single forward pass for all texts\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        predicted_class_ids = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Process results for each text\n",
    "    results = []\n",
    "    for i, text in enumerate(texts):\n",
    "        predicted_class_id = predicted_class_ids[i].item()\n",
    "        predicted_label = id2label[predicted_class_id]\n",
    "        confidence = probabilities[i][predicted_class_id].item()\n",
    "        \n",
    "        # Binary classification\n",
    "        binary_label, binary_text = map_to_binary(predicted_label)\n",
    "        \n",
    "        result = {\n",
    "            'text': text,\n",
    "            'predicted_class_id': predicted_class_id,\n",
    "            'predicted_label': predicted_label,\n",
    "            'confidence': confidence,\n",
    "            'binary_label': binary_label,\n",
    "            'binary_text': binary_text\n",
    "        }\n",
    "        \n",
    "        if return_probabilities:\n",
    "            all_probs = {id2label[j]: prob.item() for j, prob in enumerate(probabilities[i])}\n",
    "            result['all_probabilities'] = all_probs\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def predict_batch(texts, return_probabilities=False):\n",
    "    \"\"\"\n",
    "    FAKE batch prediction - just loops through predict_single (kept for compatibility)\n",
    "    Use predict_batch_real() for true batching\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        result = predict_single(text, return_probabilities)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "def display_prediction(result, true_label=None):\n",
    "    \"\"\"\n",
    "    Display prediction results in a nice format\n",
    "    \n",
    "    Args:\n",
    "        result (dict): Prediction result from predict_single or predict_batch_real\n",
    "        true_label (str, optional): Ground truth label for comparison\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“ Text: '{result['text']}'\")\n",
    "    print(f\"ğŸ·ï¸ Predicted: {result['predicted_label']} (ID: {result['predicted_class_id']})\")\n",
    "    # Show ground truth if provided\n",
    "    if true_label is not None:\n",
    "        is_correct = result['predicted_label'] == true_label\n",
    "        status_icon = \"âœ…\" if is_correct else \"âŒ\"\n",
    "        print(f\"ğŸ¯ True Label: {true_label} {status_icon}\")\n",
    "        if not is_correct:\n",
    "            print(f\"   True Description: {LABEL_DESCRIPTIONS.get(true_label, 'Unknown')}\")\n",
    "            true_binary = map_to_binary(true_label)[1]\n",
    "            pred_binary = result['binary_text']\n",
    "            binary_correct = \"âœ…\" if true_binary == pred_binary else \"âŒ\"\n",
    "            print(f\"   Binary: {true_binary} vs {pred_binary} {binary_correct}\")\n",
    "    \n",
    "    print(f\"ğŸ“Š Confidence: {result['confidence']:.4f}\")\n",
    "    print(f\"ğŸ”„ Binary Classification: {result['binary_text']}\")\n",
    "    print(f\"ğŸ’¡ Description: {LABEL_DESCRIPTIONS[result['predicted_label']]}\")\n",
    "\n",
    "    if 'all_probabilities' in result:\n",
    "        print(f\"ğŸ“ˆ Top 5 Probabilities:\")\n",
    "        sorted_probs = sorted(result['all_probabilities'].items(), key=lambda x: x[1], reverse=True)\n",
    "        for label, prob in sorted_probs[:5]:\n",
    "            print(f\"   {label}: {prob:.4f}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "def display_batch_predictions(results, true_labels=None):\n",
    "    \"\"\"\n",
    "    Display batch prediction results\n",
    "    \n",
    "    Args:\n",
    "        results (list): List of prediction results\n",
    "        true_labels (list, optional): List of ground truth labels for comparison\n",
    "    \"\"\"\n",
    "    correct_count = 0\n",
    "    total_count = len(results)\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        true_label = true_labels[i] if true_labels and i < len(true_labels) else None\n",
    "        display_prediction(result, true_label)\n",
    "        \n",
    "        if true_label and result['predicted_label'] == true_label:\n",
    "            correct_count += 1\n",
    "    \n",
    "    # Show summary if true labels provided\n",
    "    if true_labels:\n",
    "        accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "        print(f\"ğŸ“Š BATCH SUMMARY: {correct_count}/{total_count} correct ({accuracy:.1%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40473ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Text: 'I was fine-tuning BERT'\n",
      "ğŸ·ï¸ Predicted: s (ID: 11)\n",
      "ğŸ¯ True Label: s âœ…\n",
      "ğŸ“Š Confidence: 0.7435\n",
      "ğŸ”„ Binary Classification: content\n",
      "ğŸ’¡ Description: Statement\n",
      "ğŸ“ˆ Top 5 Probabilities:\n",
      "   s: 0.7435\n",
      "   %: 0.1541\n",
      "   qy: 0.0463\n",
      "   fg: 0.0297\n",
      "   fh: 0.0133\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "x = predict_single(\"I was fine-tuning BERT\", return_probabilities=True)\n",
    "display_prediction(x, true_label=\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f692f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Text: 'okay'\n",
      "ğŸ·ï¸ Predicted: fg (ID: 2)\n",
      "ğŸ¯ True Label: fg âœ…\n",
      "ğŸ“Š Confidence: 0.5002\n",
      "ğŸ”„ Binary Classification: non-content\n",
      "ğŸ’¡ Description: Floor Grabber (taking the floor)\n",
      "ğŸ“ˆ Top 5 Probabilities:\n",
      "   fg: 0.5002\n",
      "   %: 0.2218\n",
      "   s: 0.1139\n",
      "   fh: 0.1107\n",
      "   b: 0.0367\n",
      "------------------------------------------------------------\n",
      "ğŸ“ Text: 'i think that's a good idea'\n",
      "ğŸ·ï¸ Predicted: s (ID: 11)\n",
      "ğŸ¯ True Label: s âœ…\n",
      "ğŸ“Š Confidence: 0.8306\n",
      "ğŸ”„ Binary Classification: content\n",
      "ğŸ’¡ Description: Statement\n",
      "ğŸ“ˆ Top 5 Probabilities:\n",
      "   s: 0.8306\n",
      "   fg: 0.0588\n",
      "   %: 0.0479\n",
      "   fh: 0.0405\n",
      "   qy: 0.0150\n",
      "------------------------------------------------------------\n",
      "ğŸ“ Text: 'uh-huh'\n",
      "ğŸ·ï¸ Predicted: b (ID: 1)\n",
      "ğŸ¯ True Label: b âœ…\n",
      "ğŸ“Š Confidence: 0.3902\n",
      "ğŸ”„ Binary Classification: non-content\n",
      "ğŸ’¡ Description: Continuer (backchannel)\n",
      "ğŸ“ˆ Top 5 Probabilities:\n",
      "   b: 0.3902\n",
      "   h: 0.2916\n",
      "   fh: 0.1061\n",
      "   %: 0.1025\n",
      "   s: 0.0649\n",
      "------------------------------------------------------------\n",
      "ğŸ“ Text: 'what do you think about this?'\n",
      "ğŸ·ï¸ Predicted: qw (ID: 9)\n",
      "ğŸ¯ True Label: qy âŒ\n",
      "   True Description: Yes-No Question\n",
      "   Binary: content vs content âœ…\n",
      "ğŸ“Š Confidence: 0.8245\n",
      "ğŸ”„ Binary Classification: content\n",
      "ğŸ’¡ Description: Wh-Question (what, where, when, etc.)\n",
      "ğŸ“ˆ Top 5 Probabilities:\n",
      "   qw: 0.8245\n",
      "   qh: 0.0709\n",
      "   qo: 0.0652\n",
      "   qy: 0.0118\n",
      "   fg: 0.0085\n",
      "------------------------------------------------------------\n",
      "ğŸ“ Text: 'how are you doing?'\n",
      "ğŸ·ï¸ Predicted: qw (ID: 9)\n",
      "ğŸ¯ True Label: qw âœ…\n",
      "ğŸ“Š Confidence: 0.8007\n",
      "ğŸ”„ Binary Classification: content\n",
      "ğŸ’¡ Description: Wh-Question (what, where, when, etc.)\n",
      "ğŸ“ˆ Top 5 Probabilities:\n",
      "   qw: 0.8007\n",
      "   qh: 0.1072\n",
      "   qo: 0.0374\n",
      "   qr: 0.0234\n",
      "   qy: 0.0125\n",
      "------------------------------------------------------------\n",
      "ğŸ“ Text: 'um'\n",
      "ğŸ·ï¸ Predicted: fh (ID: 3)\n",
      "ğŸ¯ True Label: fh âœ…\n",
      "ğŸ“Š Confidence: 0.4020\n",
      "ğŸ”„ Binary Classification: non-content\n",
      "ğŸ’¡ Description: Floor Holder (keeping the floor)\n",
      "ğŸ“ˆ Top 5 Probabilities:\n",
      "   fh: 0.4020\n",
      "   h: 0.3341\n",
      "   fg: 0.2155\n",
      "   %: 0.0401\n",
      "   s: 0.0036\n",
      "------------------------------------------------------------\n",
      "ğŸ“Š BATCH SUMMARY: 5/6 correct (83.3%)\n"
     ]
    }
   ],
   "source": [
    "batch = [\n",
    "    \"okay\",  # Should be 'fg' (Floor Grabber) - non-content\n",
    "    \"i think that's a good idea\",  # Should be 's' (Statement) - content\n",
    "    \"uh-huh\",  # Should be 'b' (Continuer) - non-content  \n",
    "    \"what do you think about this?\",  # Should be 'qy' (Yes-No Question) - content\n",
    "    \"how are you doing?\",  # Should be 'qw' (Wh-Question) - content\n",
    "    \"um\",  # Should be 'fh' (Floor Holder) - non-content\n",
    "]\n",
    "labels = [\n",
    "    \"fg\",\n",
    "    \"s\",\n",
    "    \"b\",\n",
    "    \"qy\",\n",
    "    \"qw\",\n",
    "    \"fh\",\n",
    "]\n",
    "\n",
    "\n",
    "predictions = predict_batch_real(batch, return_probabilities=True)\n",
    "display_batch_predictions(predictions,  true_labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daba75f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing model with example utterances:\n",
      "============================================================\n",
      "ğŸ“ Text: 'okay'\n",
      "ğŸ·ï¸ Predicted: fg (ID: 2)\n",
      "ğŸ“Š Confidence: 0.5002\n",
      "ğŸ”„ Binary Classification: non-content\n",
      "ğŸ’¡ Description: Floor Grabber (taking the floor)\n",
      "ğŸ“ˆ Top 5 Probabilities:\n",
      "   fg: 0.5002\n",
      "   %: 0.2218\n",
      "   s: 0.1139\n",
      "   fh: 0.1107\n",
      "   b: 0.0367\n",
      "------------------------------------------------------------\n",
      "ğŸ“ Text: 'i think that's a good idea'\n",
      "ğŸ·ï¸ Predicted: s (ID: 11)\n",
      "ğŸ“Š Confidence: 0.8306\n",
      "ğŸ”„ Binary Classification: content\n",
      "ğŸ’¡ Description: Statement\n",
      "ğŸ“ˆ Top 5 Probabilities:\n",
      "   s: 0.8306\n",
      "   fg: 0.0588\n",
      "   %: 0.0479\n",
      "   fh: 0.0405\n",
      "   qy: 0.0150\n",
      "------------------------------------------------------------\n",
      "ğŸ“ Text: 'uh-huh'\n",
      "ğŸ·ï¸ Predicted: b (ID: 1)\n",
      "ğŸ“Š Confidence: 0.3902\n",
      "ğŸ”„ Binary Classification: non-content\n",
      "ğŸ’¡ Description: Continuer (backchannel)\n",
      "ğŸ“ˆ Top 5 Probabilities:\n",
      "   b: 0.3902\n",
      "   h: 0.2916\n",
      "   fh: 0.1061\n",
      "   %: 0.1025\n",
      "   s: 0.0649\n",
      "------------------------------------------------------------\n",
      "ğŸ“ Text: 'what do you think about this?'\n",
      "ğŸ·ï¸ Predicted: qw (ID: 9)\n",
      "ğŸ“Š Confidence: 0.8245\n",
      "ğŸ”„ Binary Classification: content\n",
      "ğŸ’¡ Description: Wh-Question (what, where, when, etc.)\n",
      "ğŸ“ˆ Top 5 Probabilities:\n",
      "   qw: 0.8245\n",
      "   qh: 0.0709\n",
      "   qo: 0.0652\n",
      "   qy: 0.0118\n",
      "   fg: 0.0085\n",
      "------------------------------------------------------------\n",
      "ğŸ“ Text: 'how are you doing?'\n",
      "ğŸ·ï¸ Predicted: qw (ID: 9)\n",
      "ğŸ“Š Confidence: 0.8007\n",
      "ğŸ”„ Binary Classification: content\n",
      "ğŸ’¡ Description: Wh-Question (what, where, when, etc.)\n",
      "ğŸ“ˆ Top 5 Probabilities:\n",
      "   qw: 0.8007\n",
      "   qh: 0.1072\n",
      "   qo: 0.0374\n",
      "   qr: 0.0234\n",
      "   qy: 0.0125\n",
      "------------------------------------------------------------\n",
      "ğŸ“ Text: 'um'\n",
      "ğŸ·ï¸ Predicted: fh (ID: 3)\n",
      "ğŸ“Š Confidence: 0.4020\n",
      "ğŸ”„ Binary Classification: non-content\n",
      "ğŸ’¡ Description: Floor Holder (keeping the floor)\n",
      "ğŸ“ˆ Top 5 Probabilities:\n",
      "   fh: 0.4020\n",
      "   h: 0.3341\n",
      "   fg: 0.2155\n",
      "   %: 0.0401\n",
      "   s: 0.0036\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the model with example utterances\n",
    "test_utterances = [\n",
    "    \"okay\",  # Should be 'fg' (Floor Grabber) - non-content\n",
    "    \"i think that's a good idea\",  # Should be 's' (Statement) - content\n",
    "    \"uh-huh\",  # Should be 'b' (Continuer) - non-content  \n",
    "    \"what do you think about this?\",  # Should be 'qy' (Yes-No Question) - content\n",
    "    \"how are you doing?\",  # Should be 'qw' (Wh-Question) - content\n",
    "    \"um\",  # Should be 'fh' (Floor Holder) - non-content\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª Testing model with example utterances:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for utterance in test_utterances:\n",
    "    result = predict_single(utterance, return_probabilities=True)\n",
    "    display_prediction(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14c0b613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Comparing batch processing methods with 50 texts:\n",
      "======================================================================\n",
      "âš¡ FAKE Batch (loops): 0.212s\n",
      "ğŸš€ REAL Batch (vectorized): 0.015s\n",
      "ğŸ’¨ Speedup: 13.9x faster!\n",
      "âœ… Results identical: True\n",
      "ğŸ“Š Accuracy: 40/50 = 80.0%\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ Always use predict_batch_real() for better performance!\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate TRUE batch processing vs FAKE batch processing\n",
    "import time\n",
    "\n",
    "# Test data\n",
    "test_texts = [\n",
    "    \"okay let's start\",\n",
    "    \"i think this is important\", \n",
    "    \"uh-huh\",\n",
    "    \"what should we do next?\",\n",
    "    \"how about this approach?\",\n",
    "    \"um let me think\",\n",
    "    \"that sounds good\",\n",
    "    \"yes exactly\",\n",
    "    \"are you sure about that?\",\n",
    "    \"maybe we should consider\"\n",
    "] * 5  # 50 texts total\n",
    "\n",
    "test_labels = [\"fg\", \"s\", \"b\", \"qw\", \"qw\", \"fh\", \"s\", \"b\", \"qy\", \"s\"] * 5\n",
    "\n",
    "print(f\"ğŸ§ª Comparing batch processing methods with {len(test_texts)} texts:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# FAKE batch processing (loops through predict_single)\n",
    "start_time = time.time()\n",
    "fake_results = predict_batch(test_texts, return_probabilities=False)\n",
    "fake_time = time.time() - start_time\n",
    "\n",
    "# TRUE batch processing (single forward pass)  \n",
    "start_time = time.time()\n",
    "real_results = predict_batch_real(test_texts, return_probabilities=False)\n",
    "real_time = time.time() - start_time\n",
    "\n",
    "print(f\"âš¡ FAKE Batch (loops): {fake_time:.3f}s\")\n",
    "print(f\"ğŸš€ REAL Batch (vectorized): {real_time:.3f}s\")\n",
    "print(f\"ğŸ’¨ Speedup: {fake_time/real_time:.1f}x faster!\")\n",
    "\n",
    "# Verify results are identical\n",
    "predictions_match = all(\n",
    "    fake['predicted_label'] == real['predicted_label'] \n",
    "    for fake, real in zip(fake_results, real_results)\n",
    ")\n",
    "print(f\"âœ… Results identical: {predictions_match}\")\n",
    "\n",
    "# Show accuracy for both methods\n",
    "fake_correct = sum(1 for i, result in enumerate(fake_results) \n",
    "                   if result['predicted_label'] == test_labels[i])\n",
    "real_correct = sum(1 for i, result in enumerate(real_results) \n",
    "                   if result['predicted_label'] == test_labels[i])\n",
    "\n",
    "print(f\"ğŸ“Š Accuracy: {fake_correct}/{len(test_texts)} = {fake_correct/len(test_texts):.1%}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ Always use predict_batch_real() for better performance!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
